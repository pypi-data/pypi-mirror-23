{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "nbsphinx": "hidden"
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "os.sys.path.insert(0, '/home/schirrmr/braindecode/code/braindecode/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Read and Decode BBCI Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "This tutorial shows how to read and decode BBCI data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Setup logging to see outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "logging.basicConfig(format='%(asctime)s %(levelname)s : %(message)s',\n",
    "                     level=logging.DEBUG, stream=sys.stdout)\n",
    "log = logging.getLogger()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Load and preprocess data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First set the filename and the sensors you want to load. If you set\n",
    "\n",
    "```python\n",
    "load_sensor_names=None\n",
    "```\n",
    "\n",
    "or just remove the parameter from the function call, all sensors will be loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating RawArray with float64 data, n_channels=3, n_times=3451320\n",
      "    Range : 0 ... 3451319 =      0.000 ...  6902.638 secs\n",
      "Ready.\n"
     ]
    }
   ],
   "source": [
    "from braindecode.datasets.bbci import BBCIDataset\n",
    "train_filename = '/home/schirrmr/data/BBCI-without-last-runs/BhNoMoSc1S001R01_ds10_1-12.BBCI.mat'\n",
    "cnt = BBCIDataset(train_filename, load_sensor_names=['C3', 'CPz', 'C4']).load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing on continous data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First remove the stimulus channel, than apply any preprocessing you like. There are some very few directions available from Braindecode, such as resample_cnt. But you can apply any function on the chan x time matrix of the mne raw object (`cnt` in the code) by calling `mne_apply` with two arguments:\n",
    "\n",
    "1. Your function (2d-array-> 2darray), that transforms the channel x timesteps data array\n",
    "2. the Raw data object from mne itself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-07-04 23:29:57,437 WARNING : This is not causal, uses future data....\n",
      "2017-07-04 23:29:57,438 INFO : Resampling from 500.000000 to 250.000000 Hz.\n",
      "Creating RawArray with float64 data, n_channels=3, n_times=1725660\n",
      "    Range : 0 ... 1725659 =      0.000 ...  6902.636 secs\n",
      "Ready.\n"
     ]
    }
   ],
   "source": [
    "from braindecode.mne_ext.signalproc import resample_cnt, mne_apply\n",
    "from braindecode.datautil.signalproc import exponential_running_standardize\n",
    "# Remove stimulus channel\n",
    "cnt = cnt.drop_channels(['STI 014'])\n",
    "cnt = resample_cnt(cnt, 250)\n",
    "# mne apply will apply the function to the data (a 2d-numpy-array)\n",
    "# have to transpose data back and forth, since\n",
    "# exponential_running_standardize expects time x chans order\n",
    "# while mne object has chans x time order\n",
    "cnt = mne_apply(lambda a: exponential_running_standardize(\n",
    "    a.T, init_block_size=1000,factor_new=0.001, eps=1e-4).T,\n",
    "    cnt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform to epoched dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Braindecode supplies the `create_signal_target_from_raw_mne` function, which will transform the mne raw object into a `SignalAndTarget` object for use in Braindecode.\n",
    "`name_to_code` should be an `OrderedDict` that maps class names to either one or a list of marker codes for that class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-07-04 23:29:59,283 INFO : Trial per class:\n",
      "Counter({'Feet': 225, 'Rest': 224, 'Right': 224, 'Left': 224})\n"
     ]
    }
   ],
   "source": [
    "from braindecode.datautil.trial_segment import create_signal_target_from_raw_mne\n",
    "from collections import OrderedDict\n",
    "# can also give lists of marker codes in case a class has multiple marker codes...\n",
    "name_to_code = OrderedDict([('Right', 1), ('Left', 2), ('Rest', 3), ('Feet', 4)])\n",
    "segment_ival_ms = [-500,4000]\n",
    "\n",
    "train_set = create_signal_target_from_raw_mne(cnt, name_to_code, segment_ival_ms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Same for test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating RawArray with float64 data, n_channels=3, n_times=617090\n",
      "    Range : 0 ... 617089 =      0.000 ...  1234.178 secs\n",
      "Ready.\n",
      "2017-07-04 23:29:59,555 WARNING : This is not causal, uses future data....\n",
      "2017-07-04 23:29:59,556 INFO : Resampling from 500.000000 to 250.000000 Hz.\n",
      "Creating RawArray with float64 data, n_channels=3, n_times=308545\n",
      "    Range : 0 ... 308544 =      0.000 ...  1234.176 secs\n",
      "Ready.\n",
      "2017-07-04 23:29:59,865 INFO : Trial per class:\n",
      "Counter({'Feet': 40, 'Rest': 40, 'Right': 40, 'Left': 40})\n"
     ]
    }
   ],
   "source": [
    "test_filename = '/home/schirrmr/data/BBCI-only-last-runs/BhNoMoSc1S001R13_ds10_1-2BBCI.mat'\n",
    "cnt = BBCIDataset(test_filename, load_sensor_names=['C3', 'CPz', 'C4']).load()\n",
    "# Remove stimulus channel\n",
    "cnt = cnt.drop_channels(['STI 014'])\n",
    "cnt = resample_cnt(cnt, 250)\n",
    "cnt = mne_apply(lambda a: exponential_running_standardize(\n",
    "    a.T, init_block_size=1000,factor_new=0.001, eps=1e-4).T,\n",
    "    cnt)\n",
    "test_set = create_signal_target_from_raw_mne(cnt, name_to_code, segment_ival_ms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "In case of start and stop markers, provide a `name_to_stop_codes` dictionary (same as for the start codes in this example) as a final argument to `create_signal_target_from_raw_mne`. See [Read and Decode BBCI Data with Start-Stop-Markers Tutorial](BBCI_Data_Start_Stop.html)\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split off a validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from braindecode.datautil.splitters import split_into_two_sets\n",
    "\n",
    "train_set, valid_set = split_into_two_sets(train_set, first_set_fraction=0.8)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Create the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from braindecode.models.shallow_fbcsp import ShallowFBCSPNet\n",
    "from torch import nn\n",
    "from braindecode.torch_ext.util import set_random_seeds, to_dense_prediction_model\n",
    "\n",
    "# Set if you want to use GPU\n",
    "# You can also use torch.cuda.is_available() to determine if cuda is available on your machine.\n",
    "cuda = True\n",
    "set_random_seeds(seed=20170629, cuda=cuda)\n",
    "\n",
    "# This will determine how many crops are processed in parallel\n",
    "input_time_length = 800\n",
    "in_chans = 3\n",
    "n_classes = 4\n",
    "# final_conv_length determines the size of the receptive field of the ConvNet\n",
    "model = ShallowFBCSPNet(in_chans=in_chans, n_classes=n_classes, input_time_length=input_time_length,\n",
    "                        final_conv_length=30).create_network()\n",
    "to_dense_prediction_model(model)\n",
    "\n",
    "if cuda:\n",
    "    model.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Setup optimizer and iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "267 predictions per input/trial\n"
     ]
    }
   ],
   "source": [
    "from torch import optim\n",
    "import numpy as np\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "from braindecode.torch_ext.util import np_to_var\n",
    "# determine output size\n",
    "test_input = np_to_var(np.ones((2, 3, input_time_length, 1), dtype=np.float32))\n",
    "if cuda:\n",
    "    test_input = test_input.cuda()\n",
    "out = model(test_input)\n",
    "n_preds_per_input = out.cpu().data.numpy().shape[2]\n",
    "print(\"{:d} predictions per input/trial\".format(n_preds_per_input))\n",
    "\n",
    "from braindecode.datautil.iterators import CropsFromTrialsIterator\n",
    "iterator = CropsFromTrialsIterator(batch_size=32,input_time_length=input_time_length,\n",
    "                                  n_preds_per_input=n_preds_per_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Setup Monitors, Loss function, Stop Criteria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from braindecode.experiments.experiment import Experiment\n",
    "from braindecode.experiments.monitors import RuntimeMonitor, LossMonitor, CroppedTrialMisclassMonitor, MisclassMonitor\n",
    "from braindecode.experiments.stopcriteria import MaxEpochs\n",
    "import torch.nn.functional as F\n",
    "import torch as th\n",
    "from braindecode.torch_ext.modules import Expression\n",
    "\n",
    "\n",
    "loss_function = lambda preds, targets: F.nll_loss(th.mean(preds, dim=2)[:,:,0], targets)\n",
    "\n",
    "model_constraint = None\n",
    "monitors = [LossMonitor(), MisclassMonitor(col_suffix='sample_misclass'),\n",
    "            CroppedTrialMisclassMonitor(input_time_length), RuntimeMonitor(),]\n",
    "stop_criterion = MaxEpochs(20)\n",
    "exp = Experiment(model, train_set, valid_set, test_set, iterator, loss_function, optimizer, model_constraint,\n",
    "          monitors, stop_criterion, remember_best_column='valid_misclass',\n",
    "          run_after_early_stop=True, batch_modifier=None, cuda=cuda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Run experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-07-04 23:30:04,072 INFO : Run until first stop...\n",
      "2017-07-04 23:30:04,726 INFO : Epoch 0\n",
      "2017-07-04 23:30:04,727 INFO : train_loss                7.89184\n",
      "2017-07-04 23:30:04,728 INFO : valid_loss                7.72731\n",
      "2017-07-04 23:30:04,728 INFO : test_loss                 7.75617\n",
      "2017-07-04 23:30:04,729 INFO : train_sample_misclass     0.75013\n",
      "2017-07-04 23:30:04,730 INFO : valid_sample_misclass     0.74856\n",
      "2017-07-04 23:30:04,730 INFO : test_sample_misclass      0.75115\n",
      "2017-07-04 23:30:04,731 INFO : train_misclass            0.75070\n",
      "2017-07-04 23:30:04,732 INFO : valid_misclass            0.74860\n",
      "2017-07-04 23:30:04,732 INFO : test_misclass             0.75000\n",
      "2017-07-04 23:30:04,733 INFO : runtime                   0.00000\n",
      "2017-07-04 23:30:04,733 INFO : \n",
      "2017-07-04 23:30:04,735 INFO : New best valid_misclass: 0.748603\n",
      "2017-07-04 23:30:04,735 INFO : \n",
      "2017-07-04 23:30:06,087 INFO : Epoch 1\n",
      "2017-07-04 23:30:06,088 INFO : train_loss                0.79477\n",
      "2017-07-04 23:30:06,089 INFO : valid_loss                0.79171\n",
      "2017-07-04 23:30:06,089 INFO : test_loss                 0.83595\n",
      "2017-07-04 23:30:06,090 INFO : train_sample_misclass     0.36904\n",
      "2017-07-04 23:30:06,091 INFO : valid_sample_misclass     0.37348\n",
      "2017-07-04 23:30:06,091 INFO : test_sample_misclass      0.43010\n",
      "2017-07-04 23:30:06,092 INFO : train_misclass            0.30223\n",
      "2017-07-04 23:30:06,093 INFO : valid_misclass            0.26257\n",
      "2017-07-04 23:30:06,093 INFO : test_misclass             0.35625\n",
      "2017-07-04 23:30:06,094 INFO : runtime                   1.40718\n",
      "2017-07-04 23:30:06,094 INFO : \n",
      "2017-07-04 23:30:06,096 INFO : New best valid_misclass: 0.262570\n",
      "2017-07-04 23:30:06,097 INFO : \n",
      "2017-07-04 23:30:07,404 INFO : Epoch 2\n",
      "2017-07-04 23:30:07,405 INFO : train_loss                0.68187\n",
      "2017-07-04 23:30:07,406 INFO : valid_loss                0.65910\n",
      "2017-07-04 23:30:07,407 INFO : test_loss                 0.74160\n",
      "2017-07-04 23:30:07,408 INFO : train_sample_misclass     0.29303\n",
      "2017-07-04 23:30:07,409 INFO : valid_sample_misclass     0.30269\n",
      "2017-07-04 23:30:07,410 INFO : test_sample_misclass      0.37348\n",
      "2017-07-04 23:30:07,410 INFO : train_misclass            0.22423\n",
      "2017-07-04 23:30:07,411 INFO : valid_misclass            0.20670\n",
      "2017-07-04 23:30:07,411 INFO : test_misclass             0.30000\n",
      "2017-07-04 23:30:07,412 INFO : runtime                   1.33519\n",
      "2017-07-04 23:30:07,413 INFO : \n",
      "2017-07-04 23:30:07,414 INFO : New best valid_misclass: 0.206704\n",
      "2017-07-04 23:30:07,415 INFO : \n",
      "2017-07-04 23:30:08,746 INFO : Epoch 3\n",
      "2017-07-04 23:30:08,748 INFO : train_loss                0.65562\n",
      "2017-07-04 23:30:08,749 INFO : valid_loss                0.68534\n",
      "2017-07-04 23:30:08,750 INFO : test_loss                 0.81763\n",
      "2017-07-04 23:30:08,751 INFO : train_sample_misclass     0.27891\n",
      "2017-07-04 23:30:08,752 INFO : valid_sample_misclass     0.32183\n",
      "2017-07-04 23:30:08,752 INFO : test_sample_misclass      0.43740\n",
      "2017-07-04 23:30:08,753 INFO : train_misclass            0.20334\n",
      "2017-07-04 23:30:08,754 INFO : valid_misclass            0.26816\n",
      "2017-07-04 23:30:08,755 INFO : test_misclass             0.35625\n",
      "2017-07-04 23:30:08,756 INFO : runtime                   1.31997\n",
      "2017-07-04 23:30:08,756 INFO : \n",
      "2017-07-04 23:30:10,097 INFO : Epoch 4\n",
      "2017-07-04 23:30:10,101 INFO : train_loss                0.60710\n",
      "2017-07-04 23:30:10,102 INFO : valid_loss                0.59891\n",
      "2017-07-04 23:30:10,102 INFO : test_loss                 0.73585\n",
      "2017-07-04 23:30:10,103 INFO : train_sample_misclass     0.26582\n",
      "2017-07-04 23:30:10,104 INFO : valid_sample_misclass     0.26849\n",
      "2017-07-04 23:30:10,105 INFO : test_sample_misclass      0.35936\n",
      "2017-07-04 23:30:10,106 INFO : train_misclass            0.20334\n",
      "2017-07-04 23:30:10,107 INFO : valid_misclass            0.18436\n",
      "2017-07-04 23:30:10,107 INFO : test_misclass             0.29375\n",
      "2017-07-04 23:30:10,108 INFO : runtime                   1.35176\n",
      "2017-07-04 23:30:10,109 INFO : \n",
      "2017-07-04 23:30:10,111 INFO : New best valid_misclass: 0.184358\n",
      "2017-07-04 23:30:10,112 INFO : \n",
      "2017-07-04 23:30:11,441 INFO : Epoch 5\n",
      "2017-07-04 23:30:11,443 INFO : train_loss                0.66732\n",
      "2017-07-04 23:30:11,444 INFO : valid_loss                0.62454\n",
      "2017-07-04 23:30:11,444 INFO : test_loss                 0.72177\n",
      "2017-07-04 23:30:11,445 INFO : train_sample_misclass     0.28037\n",
      "2017-07-04 23:30:11,445 INFO : valid_sample_misclass     0.29130\n",
      "2017-07-04 23:30:11,446 INFO : test_sample_misclass      0.32656\n",
      "2017-07-04 23:30:11,447 INFO : train_misclass            0.22423\n",
      "2017-07-04 23:30:11,447 INFO : valid_misclass            0.22905\n",
      "2017-07-04 23:30:11,448 INFO : test_misclass             0.25625\n",
      "2017-07-04 23:30:11,448 INFO : runtime                   1.35574\n",
      "2017-07-04 23:30:11,449 INFO : \n",
      "2017-07-04 23:30:12,758 INFO : Epoch 6\n",
      "2017-07-04 23:30:12,759 INFO : train_loss                0.61130\n",
      "2017-07-04 23:30:12,760 INFO : valid_loss                0.65038\n",
      "2017-07-04 23:30:12,760 INFO : test_loss                 0.72129\n",
      "2017-07-04 23:30:12,761 INFO : train_sample_misclass     0.25898\n",
      "2017-07-04 23:30:12,762 INFO : valid_sample_misclass     0.28879\n",
      "2017-07-04 23:30:12,762 INFO : test_sample_misclass      0.34085\n",
      "2017-07-04 23:30:12,763 INFO : train_misclass            0.19916\n",
      "2017-07-04 23:30:12,763 INFO : valid_misclass            0.21229\n",
      "2017-07-04 23:30:12,764 INFO : test_misclass             0.28125\n",
      "2017-07-04 23:30:12,765 INFO : runtime                   1.32989\n",
      "2017-07-04 23:30:12,765 INFO : \n",
      "2017-07-04 23:30:14,067 INFO : Epoch 7\n",
      "2017-07-04 23:30:14,068 INFO : train_loss                0.58138\n",
      "2017-07-04 23:30:14,069 INFO : valid_loss                0.56076\n",
      "2017-07-04 23:30:14,070 INFO : test_loss                 0.65182\n",
      "2017-07-04 23:30:14,070 INFO : train_sample_misclass     0.23837\n",
      "2017-07-04 23:30:14,071 INFO : valid_sample_misclass     0.25394\n",
      "2017-07-04 23:30:14,071 INFO : test_sample_misclass      0.30634\n",
      "2017-07-04 23:30:14,072 INFO : train_misclass            0.17967\n",
      "2017-07-04 23:30:14,073 INFO : valid_misclass            0.17318\n",
      "2017-07-04 23:30:14,073 INFO : test_misclass             0.25000\n",
      "2017-07-04 23:30:14,074 INFO : runtime                   1.31053\n",
      "2017-07-04 23:30:14,074 INFO : \n",
      "2017-07-04 23:30:14,076 INFO : New best valid_misclass: 0.173184\n",
      "2017-07-04 23:30:14,077 INFO : \n",
      "2017-07-04 23:30:15,381 INFO : Epoch 8\n",
      "2017-07-04 23:30:15,382 INFO : train_loss                0.51176\n",
      "2017-07-04 23:30:15,383 INFO : valid_loss                0.55876\n",
      "2017-07-04 23:30:15,384 INFO : test_loss                 0.65875\n",
      "2017-07-04 23:30:15,384 INFO : train_sample_misclass     0.22266\n",
      "2017-07-04 23:30:15,385 INFO : valid_sample_misclass     0.25092\n",
      "2017-07-04 23:30:15,385 INFO : test_sample_misclass      0.32118\n",
      "2017-07-04 23:30:15,386 INFO : train_misclass            0.13788\n",
      "2017-07-04 23:30:15,387 INFO : valid_misclass            0.17318\n",
      "2017-07-04 23:30:15,387 INFO : test_misclass             0.21875\n",
      "2017-07-04 23:30:15,388 INFO : runtime                   1.31316\n",
      "2017-07-04 23:30:15,388 INFO : \n",
      "2017-07-04 23:30:15,390 INFO : New best valid_misclass: 0.173184\n",
      "2017-07-04 23:30:15,391 INFO : \n",
      "2017-07-04 23:30:16,693 INFO : Epoch 9\n",
      "2017-07-04 23:30:16,694 INFO : train_loss                0.52610\n",
      "2017-07-04 23:30:16,695 INFO : valid_loss                0.57157\n",
      "2017-07-04 23:30:16,696 INFO : test_loss                 0.65935\n",
      "2017-07-04 23:30:16,696 INFO : train_sample_misclass     0.20596\n",
      "2017-07-04 23:30:16,697 INFO : valid_sample_misclass     0.26240\n",
      "2017-07-04 23:30:16,698 INFO : test_sample_misclass      0.32477\n",
      "2017-07-04 23:30:16,698 INFO : train_misclass            0.13370\n",
      "2017-07-04 23:30:16,699 INFO : valid_misclass            0.16201\n",
      "2017-07-04 23:30:16,699 INFO : test_misclass             0.20625\n",
      "2017-07-04 23:30:16,700 INFO : runtime                   1.31230\n",
      "2017-07-04 23:30:16,701 INFO : \n",
      "2017-07-04 23:30:16,702 INFO : New best valid_misclass: 0.162011\n",
      "2017-07-04 23:30:16,703 INFO : \n",
      "2017-07-04 23:30:18,015 INFO : Epoch 10\n",
      "2017-07-04 23:30:18,016 INFO : train_loss                0.51490\n",
      "2017-07-04 23:30:18,017 INFO : valid_loss                0.52639\n",
      "2017-07-04 23:30:18,018 INFO : test_loss                 0.63949\n",
      "2017-07-04 23:30:18,018 INFO : train_sample_misclass     0.20312\n",
      "2017-07-04 23:30:18,019 INFO : valid_sample_misclass     0.22577\n",
      "2017-07-04 23:30:18,020 INFO : test_sample_misclass      0.31473\n",
      "2017-07-04 23:30:18,020 INFO : train_misclass            0.13231\n",
      "2017-07-04 23:30:18,021 INFO : valid_misclass            0.16760\n",
      "2017-07-04 23:30:18,021 INFO : test_misclass             0.23750\n",
      "2017-07-04 23:30:18,022 INFO : runtime                   1.32239\n",
      "2017-07-04 23:30:18,022 INFO : \n",
      "2017-07-04 23:30:19,359 INFO : Epoch 11\n",
      "2017-07-04 23:30:19,361 INFO : train_loss                0.49356\n",
      "2017-07-04 23:30:19,362 INFO : valid_loss                0.53888\n",
      "2017-07-04 23:30:19,363 INFO : test_loss                 0.62956\n",
      "2017-07-04 23:30:19,363 INFO : train_sample_misclass     0.19936\n",
      "2017-07-04 23:30:19,364 INFO : valid_sample_misclass     0.24194\n",
      "2017-07-04 23:30:19,365 INFO : test_sample_misclass      0.31319\n",
      "2017-07-04 23:30:19,366 INFO : train_misclass            0.12256\n",
      "2017-07-04 23:30:19,367 INFO : valid_misclass            0.16201\n",
      "2017-07-04 23:30:19,368 INFO : test_misclass             0.21250\n",
      "2017-07-04 23:30:19,369 INFO : runtime                   1.31399\n",
      "2017-07-04 23:30:19,369 INFO : \n",
      "2017-07-04 23:30:19,372 INFO : New best valid_misclass: 0.162011\n",
      "2017-07-04 23:30:19,373 INFO : \n",
      "2017-07-04 23:30:20,718 INFO : Epoch 12\n",
      "2017-07-04 23:30:20,719 INFO : train_loss                0.48967\n",
      "2017-07-04 23:30:20,720 INFO : valid_loss                0.54931\n",
      "2017-07-04 23:30:20,721 INFO : test_loss                 0.65452\n",
      "2017-07-04 23:30:20,721 INFO : train_sample_misclass     0.20714\n",
      "2017-07-04 23:30:20,722 INFO : valid_sample_misclass     0.24781\n",
      "2017-07-04 23:30:20,723 INFO : test_sample_misclass      0.30963\n",
      "2017-07-04 23:30:20,724 INFO : train_misclass            0.14067\n",
      "2017-07-04 23:30:20,725 INFO : valid_misclass            0.15642\n",
      "2017-07-04 23:30:20,725 INFO : test_misclass             0.23125\n",
      "2017-07-04 23:30:20,726 INFO : runtime                   1.36793\n",
      "2017-07-04 23:30:20,727 INFO : \n",
      "2017-07-04 23:30:20,728 INFO : New best valid_misclass: 0.156425\n",
      "2017-07-04 23:30:20,729 INFO : \n",
      "2017-07-04 23:30:22,056 INFO : Epoch 13\n",
      "2017-07-04 23:30:22,057 INFO : train_loss                0.58978\n",
      "2017-07-04 23:30:22,058 INFO : valid_loss                0.60427\n",
      "2017-07-04 23:30:22,058 INFO : test_loss                 0.81804\n",
      "2017-07-04 23:30:22,059 INFO : train_sample_misclass     0.24555\n",
      "2017-07-04 23:30:22,059 INFO : valid_sample_misclass     0.26532\n",
      "2017-07-04 23:30:22,060 INFO : test_sample_misclass      0.41140\n",
      "2017-07-04 23:30:22,061 INFO : train_misclass            0.18942\n",
      "2017-07-04 23:30:22,061 INFO : valid_misclass            0.20112\n",
      "2017-07-04 23:30:22,062 INFO : test_misclass             0.33125\n",
      "2017-07-04 23:30:22,062 INFO : runtime                   1.34717\n",
      "2017-07-04 23:30:22,063 INFO : \n",
      "2017-07-04 23:30:23,383 INFO : Epoch 14\n",
      "2017-07-04 23:30:23,385 INFO : train_loss                0.48318\n",
      "2017-07-04 23:30:23,385 INFO : valid_loss                0.49395\n",
      "2017-07-04 23:30:23,386 INFO : test_loss                 0.61195\n",
      "2017-07-04 23:30:23,387 INFO : train_sample_misclass     0.19086\n",
      "2017-07-04 23:30:23,387 INFO : valid_sample_misclass     0.22468\n",
      "2017-07-04 23:30:23,388 INFO : test_sample_misclass      0.29158\n",
      "2017-07-04 23:30:23,388 INFO : train_misclass            0.11978\n",
      "2017-07-04 23:30:23,389 INFO : valid_misclass            0.14525\n",
      "2017-07-04 23:30:23,390 INFO : test_misclass             0.20625\n",
      "2017-07-04 23:30:23,390 INFO : runtime                   1.33120\n",
      "2017-07-04 23:30:23,391 INFO : \n",
      "2017-07-04 23:30:23,392 INFO : New best valid_misclass: 0.145251\n",
      "2017-07-04 23:30:23,393 INFO : \n",
      "2017-07-04 23:30:24,732 INFO : Epoch 15\n",
      "2017-07-04 23:30:24,733 INFO : train_loss                0.52088\n",
      "2017-07-04 23:30:24,734 INFO : valid_loss                0.56505\n",
      "2017-07-04 23:30:24,735 INFO : test_loss                 0.65076\n",
      "2017-07-04 23:30:24,736 INFO : train_sample_misclass     0.20715\n",
      "2017-07-04 23:30:24,737 INFO : valid_sample_misclass     0.25441\n",
      "2017-07-04 23:30:24,738 INFO : test_sample_misclass      0.32642\n",
      "2017-07-04 23:30:24,738 INFO : train_misclass            0.14903\n",
      "2017-07-04 23:30:24,739 INFO : valid_misclass            0.20112\n",
      "2017-07-04 23:30:24,740 INFO : test_misclass             0.23750\n",
      "2017-07-04 23:30:24,741 INFO : runtime                   1.33243\n",
      "2017-07-04 23:30:24,742 INFO : \n",
      "2017-07-04 23:30:26,082 INFO : Epoch 16\n",
      "2017-07-04 23:30:26,083 INFO : train_loss                0.54214\n",
      "2017-07-04 23:30:26,084 INFO : valid_loss                0.57582\n",
      "2017-07-04 23:30:26,085 INFO : test_loss                 0.68844\n",
      "2017-07-04 23:30:26,086 INFO : train_sample_misclass     0.21816\n",
      "2017-07-04 23:30:26,086 INFO : valid_sample_misclass     0.24900\n",
      "2017-07-04 23:30:26,087 INFO : test_sample_misclass      0.33597\n",
      "2017-07-04 23:30:26,088 INFO : train_misclass            0.16992\n",
      "2017-07-04 23:30:26,089 INFO : valid_misclass            0.19553\n",
      "2017-07-04 23:30:26,090 INFO : test_misclass             0.29375\n",
      "2017-07-04 23:30:26,090 INFO : runtime                   1.35571\n",
      "2017-07-04 23:30:26,091 INFO : \n",
      "2017-07-04 23:30:27,432 INFO : Epoch 17\n",
      "2017-07-04 23:30:27,434 INFO : train_loss                0.56662\n",
      "2017-07-04 23:30:27,434 INFO : valid_loss                0.62617\n",
      "2017-07-04 23:30:27,435 INFO : test_loss                 0.71926\n",
      "2017-07-04 23:30:27,435 INFO : train_sample_misclass     0.24136\n",
      "2017-07-04 23:30:27,436 INFO : valid_sample_misclass     0.27001\n",
      "2017-07-04 23:30:27,437 INFO : test_sample_misclass      0.36318\n",
      "2017-07-04 23:30:27,437 INFO : train_misclass            0.20056\n",
      "2017-07-04 23:30:27,438 INFO : valid_misclass            0.23464\n",
      "2017-07-04 23:30:27,438 INFO : test_misclass             0.30625\n",
      "2017-07-04 23:30:27,439 INFO : runtime                   1.35640\n",
      "2017-07-04 23:30:27,440 INFO : \n",
      "2017-07-04 23:30:28,792 INFO : Epoch 18\n",
      "2017-07-04 23:30:28,793 INFO : train_loss                0.46215\n",
      "2017-07-04 23:30:28,794 INFO : valid_loss                0.52255\n",
      "2017-07-04 23:30:28,794 INFO : test_loss                 0.64409\n",
      "2017-07-04 23:30:28,795 INFO : train_sample_misclass     0.18938\n",
      "2017-07-04 23:30:28,795 INFO : valid_sample_misclass     0.23547\n",
      "2017-07-04 23:30:28,796 INFO : test_sample_misclass      0.32831\n",
      "2017-07-04 23:30:28,797 INFO : train_misclass            0.11978\n",
      "2017-07-04 23:30:28,798 INFO : valid_misclass            0.15642\n",
      "2017-07-04 23:30:28,799 INFO : test_misclass             0.23125\n",
      "2017-07-04 23:30:28,800 INFO : runtime                   1.34665\n",
      "2017-07-04 23:30:28,800 INFO : \n",
      "2017-07-04 23:30:30,169 INFO : Epoch 19\n",
      "2017-07-04 23:30:30,171 INFO : train_loss                0.46447\n",
      "2017-07-04 23:30:30,172 INFO : valid_loss                0.54443\n",
      "2017-07-04 23:30:30,173 INFO : test_loss                 0.65507\n",
      "2017-07-04 23:30:30,174 INFO : train_sample_misclass     0.19552\n",
      "2017-07-04 23:30:30,175 INFO : valid_sample_misclass     0.24735\n",
      "2017-07-04 23:30:30,176 INFO : test_sample_misclass      0.32516\n",
      "2017-07-04 23:30:30,176 INFO : train_misclass            0.11838\n",
      "2017-07-04 23:30:30,177 INFO : valid_misclass            0.15642\n",
      "2017-07-04 23:30:30,178 INFO : test_misclass             0.22500\n",
      "2017-07-04 23:30:30,179 INFO : runtime                   1.36252\n",
      "2017-07-04 23:30:30,180 INFO : \n",
      "2017-07-04 23:30:31,533 INFO : Epoch 20\n",
      "2017-07-04 23:30:31,535 INFO : train_loss                0.47758\n",
      "2017-07-04 23:30:31,536 INFO : valid_loss                0.51993\n",
      "2017-07-04 23:30:31,537 INFO : test_loss                 0.58186\n",
      "2017-07-04 23:30:31,538 INFO : train_sample_misclass     0.19120\n",
      "2017-07-04 23:30:31,539 INFO : valid_sample_misclass     0.23011\n",
      "2017-07-04 23:30:31,539 INFO : test_sample_misclass      0.27199\n",
      "2017-07-04 23:30:31,540 INFO : train_misclass            0.11838\n",
      "2017-07-04 23:30:31,541 INFO : valid_misclass            0.16760\n",
      "2017-07-04 23:30:31,542 INFO : test_misclass             0.19375\n",
      "2017-07-04 23:30:31,543 INFO : runtime                   1.37400\n",
      "2017-07-04 23:30:31,544 INFO : \n",
      "2017-07-04 23:30:31,545 INFO : Setup for second stop...\n",
      "2017-07-04 23:30:31,548 INFO : Train loss to reach 0.48318\n",
      "2017-07-04 23:30:31,549 INFO : Run until second stop...\n",
      "2017-07-04 23:30:32,291 INFO : Epoch 15\n",
      "2017-07-04 23:30:32,292 INFO : train_loss                0.48533\n",
      "2017-07-04 23:30:32,293 INFO : valid_loss                0.49395\n",
      "2017-07-04 23:30:32,294 INFO : test_loss                 0.61195\n",
      "2017-07-04 23:30:32,294 INFO : train_sample_misclass     0.19761\n",
      "2017-07-04 23:30:32,295 INFO : valid_sample_misclass     0.22468\n",
      "2017-07-04 23:30:32,296 INFO : test_sample_misclass      0.29158\n",
      "2017-07-04 23:30:32,297 INFO : train_misclass            0.12486\n",
      "2017-07-04 23:30:32,297 INFO : valid_misclass            0.14525\n",
      "2017-07-04 23:30:32,298 INFO : test_misclass             0.20625\n",
      "2017-07-04 23:30:32,299 INFO : runtime                   0.63342\n",
      "2017-07-04 23:30:32,299 INFO : \n",
      "2017-07-04 23:30:33,953 INFO : Epoch 16\n",
      "2017-07-04 23:30:33,955 INFO : train_loss                0.48958\n",
      "2017-07-04 23:30:33,956 INFO : valid_loss                0.46042\n",
      "2017-07-04 23:30:33,957 INFO : test_loss                 0.60305\n",
      "2017-07-04 23:30:33,958 INFO : train_sample_misclass     0.20106\n",
      "2017-07-04 23:30:33,959 INFO : valid_sample_misclass     0.20347\n",
      "2017-07-04 23:30:33,960 INFO : test_sample_misclass      0.28212\n",
      "2017-07-04 23:30:33,960 INFO : train_misclass            0.14493\n",
      "2017-07-04 23:30:33,961 INFO : valid_misclass            0.13966\n",
      "2017-07-04 23:30:33,962 INFO : test_misclass             0.20000\n",
      "2017-07-04 23:30:33,963 INFO : runtime                   1.68177\n",
      "2017-07-04 23:30:33,964 INFO : \n"
     ]
    }
   ],
   "source": [
    "exp.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We arrive at 80% accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to do trialwise decoding instead of cropped decoding, perform the following changes:\n",
    "\n",
    "\n",
    "Change:\n",
    "```python\n",
    "# This will determine how many crops are processed in parallel\n",
    "input_time_length = 800\n",
    "in_chans = 3\n",
    "n_classes = 4\n",
    "# final_conv_length determines the size of the receptive field of the ConvNet\n",
    "model = ShallowFBCSPNet(in_chans=in_chans, n_classes=n_classes, input_time_length=input_time_length,\n",
    "                        final_conv_length=30).create_network()\n",
    "```\n",
    "\n",
    "to:\n",
    "```python\n",
    "# This will determine how many crops are processed in parallel\n",
    "input_time_length = train_set.X.shape[2]\n",
    "in_chans = 3\n",
    "n_classes = 4\n",
    "# final_conv_length determines the size of the receptive field of the ConvNet\n",
    "model = ShallowFBCSPNet(in_chans=in_chans, n_classes=n_classes, input_time_length=input_time_length,\n",
    "                        final_conv_length='auto').create_network()\n",
    "```\n",
    "\n",
    "Remove:\n",
    "\n",
    "```python\n",
    "to_dense_prediction_model(model)\n",
    "```\n",
    "\n",
    "Remove:\n",
    "\n",
    "\n",
    "```python\n",
    "from braindecode.torch_ext.util import np_to_var\n",
    "# determine output size\n",
    "test_input = np_to_var(np.ones((2, 3, input_time_length, 1), dtype=np.float32))\n",
    "if cuda:\n",
    "    test_input = test_input.cuda()\n",
    "out = model(test_input)\n",
    "n_preds_per_input = out.cpu().data.numpy().shape[2]\n",
    "print(\"{:d} predictions per input/trial\".format(n_preds_per_input))\n",
    "```\n",
    "\n",
    "Change:\n",
    "\n",
    "```python\n",
    "from braindecode.datautil.iterators import CropsFromTrialsIterator\n",
    "iterator = CropsFromTrialsIterator(batch_size=32,input_time_length=input_time_length,\n",
    "                                  n_preds_per_input=n_preds_per_input)\n",
    "```\n",
    "\n",
    "to:\n",
    "\n",
    "```python\n",
    "from braindecode.datautil.iterators import BalancedBatchSizeIterator\n",
    "iterator = BalancedBatchSizeIterator(batch_size=32)\n",
    "```\n",
    "\n",
    "\n",
    "Change:\n",
    "\n",
    "```python\n",
    "loss_function = lambda preds, targets: F.nll_loss(th.mean(preds, dim=2)[:,:,0], targets)\n",
    "```\n",
    "\n",
    "to:\n",
    "\n",
    "```python\n",
    "loss_function = F.nll_loss\n",
    "```\n",
    "\n",
    "Change:\n",
    "\n",
    "```python\n",
    "monitors = [LossMonitor(), MisclassMonitor(col_suffix='sample_misclass'),\n",
    "            CroppedTrialMisclassMonitor(input_time_length), RuntimeMonitor(),]\n",
    "```\n",
    "\n",
    "to:\n",
    "\n",
    "```python\n",
    "monitors = [LossMonitor(), MisclassMonitor(col_suffix='misclass'), \n",
    "            RuntimeMonitor(),]\n",
    "```\n",
    "\n",
    "Resulting code can be seen at [BBCI Data Epoched](BBCI_Data_Epoched.html)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
