{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "nbsphinx": "hidden"
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "os.sys.path.insert(0, '/home/schirrmr/braindecode/code/braindecode/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Read and Decode BBCI Data with Start-Stop-Markers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "This tutorial shows how to read and decode BBCI data with start and stop markers. The data loading part is more complicated and it is advised to read the other tutorials before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Setup logging to see outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "logging.basicConfig(format='%(asctime)s %(levelname)s : %(message)s',\n",
    "                     level=logging.DEBUG, stream=sys.stdout)\n",
    "log = logging.getLogger()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Load and preprocess data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "This is a bit more complicated than before since we have to add breaks etc. Here I now opt to add breaks do all preprocessings per run and only later combine them together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from braindecode.datautil.splitters import concatenate_sets\n",
    "from braindecode.datautil.trial_segment import create_signal_target_from_raw_mne, add_breaks\n",
    "from braindecode.datasets.bbci import load_bbci_sets_from_folder\n",
    "from copy import deepcopy\n",
    "from braindecode.mne_ext.signalproc import resample_cnt, mne_apply\n",
    "from braindecode.datautil.signalproc import lowpass_cnt\n",
    "from braindecode.datautil.signalproc import exponential_running_standardize\n",
    "\n",
    "def create_cnts(folder, runs, name_to_start_code, name_to_stop_code, break_start_offset_ms,\n",
    "              break_stop_offset_ms, break_start_code, break_stop_code):\n",
    "    # Load data\n",
    "    cnts = load_bbci_sets_from_folder(folder, runs)\n",
    "    \n",
    "    # Now do some preprocessings:\n",
    "    # Adding breaks, resampling to 250 Hz, lowpass below 38, eponential standardization\n",
    "    break_start_code = -1\n",
    "    break_stop_code = -2\n",
    "    \n",
    "    new_cnts = []\n",
    "    for cnt in cnts:\n",
    "        # Only take some channels \n",
    "        #cnt = cnt.drop_channels(['STI 014']) # This would remove stimulus channel\n",
    "        cnt = cnt.pick_channels(['C3', 'CPz' 'C4'])\n",
    "        # add breaks\n",
    "        new_events = add_breaks(\n",
    "            cnt.info['events'], cnt.info['sfreq'],\n",
    "            break_start_code=break_start_code, break_stop_code=break_stop_code,\n",
    "            name_to_start_codes=name_to_start_code, name_to_stop_codes=name_to_stop_code,\n",
    "            min_break_length_ms=5000, max_break_length_ms=9000)\n",
    "        n_break_start_offset = int(cnt.info['sfreq'] * break_start_offset_ms / 1000.0)\n",
    "        n_break_stop_offset = int(cnt.info['sfreq'] * break_stop_offset_ms / 1000.0)\n",
    "        # lets add some offset to break start and stop\n",
    "        # new_events[:,2] contains event codes, new_events[:,0] the sample indices\n",
    "        # new_events[:,1] is always 0 for my loading of BBCI data\n",
    "        new_events[new_events[:,2] == break_start_code, 0] += n_break_start_offset\n",
    "        # 0.5 sec for break stop\n",
    "        new_events[new_events[:,2] == break_stop_code, 0] +=  n_break_stop_offset\n",
    "        cnt.info['events'] = new_events\n",
    "        log.info(\"Preprocessing....\")\n",
    "        cnt = mne_apply(lambda a: lowpass_cnt(a, 38,cnt.info['sfreq'], axis=1), cnt)\n",
    "        cnt = resample_cnt(cnt, 250)\n",
    "        # mne apply will apply the function to the data (a 2d-numpy-array)\n",
    "        # have to transpose data back and forth, since\n",
    "        # exponential_running_standardize expects time x chans order\n",
    "        # while mne object has chans x time order\n",
    "        cnt = mne_apply(lambda a: exponential_running_standardize(\n",
    "            a.T, init_block_size=1000,factor_new=0.001, eps=1e-4).T,\n",
    "            cnt)\n",
    "        new_cnts.append(cnt)\n",
    "    return new_cnts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-07-05 13:10:11,847 INFO : Loading /home/schirrmr/data/robot-hall/AnLa/AnLaNBD1R01-8/AnLaNBD1S001R01_1-1_250Hz.BBCI.mat\n",
      "Creating RawArray with float64 data, n_channels=64, n_times=151350\n",
      "    Range : 0 ... 151349 =      0.000 ...   605.396 secs\n",
      "Ready.\n",
      "2017-07-05 13:10:13,371 INFO : Loading /home/schirrmr/data/robot-hall/AnLa/AnLaNBD1R01-8/AnLaNBD1S001R02_1-1_250Hz.BBCI.mat\n",
      "Creating RawArray with float64 data, n_channels=64, n_times=153500\n",
      "    Range : 0 ... 153499 =      0.000 ...   613.996 secs\n",
      "Ready.\n",
      "2017-07-05 13:10:14,465 INFO : Loading /home/schirrmr/data/robot-hall/AnLa/AnLaNBD1R01-8/AnLaNBD1S001R03_1-1_250Hz.BBCI.mat\n",
      "Creating RawArray with float64 data, n_channels=64, n_times=180700\n",
      "    Range : 0 ... 180699 =      0.000 ...   722.796 secs\n",
      "Ready.\n",
      "2017-07-05 13:10:15,825 INFO : Preprocessing....\n",
      "2017-07-05 13:10:15,831 INFO : Just copying data, no resampling, since new sampling rate same.\n",
      "2017-07-05 13:10:15,925 INFO : Preprocessing....\n",
      "2017-07-05 13:10:15,930 INFO : Just copying data, no resampling, since new sampling rate same.\n",
      "2017-07-05 13:10:16,033 INFO : Preprocessing....\n",
      "2017-07-05 13:10:16,038 INFO : Just copying data, no resampling, since new sampling rate same.\n"
     ]
    }
   ],
   "source": [
    "from collections import OrderedDict\n",
    "name_to_start_code = OrderedDict([('Right Hand', 1), ('Feet', 4),\n",
    "            ('Rotation', 8), ('Words', [10])])\n",
    "\n",
    "name_to_stop_code = OrderedDict([('Right Hand', [20,21,22,23,24,28,30]),\n",
    "        ('Feet', [20,21,22,23,24,28,30]),\n",
    "        ('Rotation', [20,21,22,23,24,28,30]), \n",
    "        ('Words', [20,21,22,23,24,28,30])])\n",
    "\n",
    "break_start_offset_ms = 1000\n",
    "break_stop_offset_ms = -500\n",
    "# pick some numbers that were not used before/do not exist in markers\n",
    "break_start_code = -1\n",
    "break_stop_code = -2\n",
    "train_runs = [1,2,3]\n",
    "train_cnts = create_cnts('/home/schirrmr/data/robot-hall/AnLa/AnLaNBD1R01-8/', \n",
    "                         train_runs,\n",
    "                         name_to_start_code,\n",
    "                         name_to_stop_code, break_start_offset_ms,\n",
    "                         break_stop_offset_ms, break_start_code, break_stop_code)\n",
    "\n",
    "name_to_code_with_breaks = deepcopy(name_to_start_code)\n",
    "name_to_code_with_breaks['Break'] = break_start_code\n",
    "name_to_stop_code_with_breaks = deepcopy(name_to_stop_code)\n",
    "name_to_stop_code_with_breaks['Break'] = break_stop_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-07-05 13:10:16,096 INFO : Loading /home/schirrmr/data/robot-hall/AnLa/AnLaNBD1R09-10/AnLaNBD1S001R09_1-1_250Hz.BBCI.mat\n",
      "Creating RawArray with float64 data, n_channels=64, n_times=152050\n",
      "    Range : 0 ... 152049 =      0.000 ...   608.196 secs\n",
      "Ready.\n",
      "2017-07-05 13:10:17,176 INFO : Loading /home/schirrmr/data/robot-hall/AnLa/AnLaNBD1R09-10/AnLaNBD1S001R10_1-1_250Hz.BBCI.mat\n",
      "Creating RawArray with float64 data, n_channels=64, n_times=151100\n",
      "    Range : 0 ... 151099 =      0.000 ...   604.396 secs\n",
      "Ready.\n",
      "2017-07-05 13:10:18,307 INFO : Preprocessing....\n",
      "2017-07-05 13:10:18,311 INFO : Just copying data, no resampling, since new sampling rate same.\n",
      "2017-07-05 13:10:18,402 INFO : Preprocessing....\n",
      "2017-07-05 13:10:18,406 INFO : Just copying data, no resampling, since new sampling rate same.\n"
     ]
    }
   ],
   "source": [
    "test_runs = [9,10]\n",
    "test_cnts = create_cnts('/home/schirrmr/data/robot-hall/AnLa/AnLaNBD1R09-10/', test_runs, name_to_start_code,\n",
    "           name_to_stop_code, break_start_offset_ms,\n",
    "              break_stop_offset_ms, break_start_code, break_stop_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Create the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We already create the model now, since we need to know the receptive field size for properly cutting out the data to predict. We need to cut out data starting at -receptive_field_size samples before the first sample we want to predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "132 predictions per input/trial\n",
      "Receptive field: 518/2072.00 (samples/ms)\n"
     ]
    }
   ],
   "source": [
    "from braindecode.models.shallow_fbcsp import ShallowFBCSPNet\n",
    "from torch import nn\n",
    "from braindecode.torch_ext.util import set_random_seeds\n",
    "from braindecode.models.util import to_dense_prediction_model\n",
    "\n",
    "# Set if you want to use GPU\n",
    "# You can also use torch.cuda.is_available() to determine if cuda is available on your machine.\n",
    "cuda = True\n",
    "set_random_seeds(seed=20170629, cuda=cuda)\n",
    "\n",
    "# This will determine how many crops are processed in parallel\n",
    "input_time_length = 650\n",
    "in_chans = train_cnts[0].get_data().shape[0]\n",
    "# final_conv_length determines the size of the receptive field of the ConvNet\n",
    "model = ShallowFBCSPNet(in_chans=in_chans, n_classes=5, input_time_length=input_time_length,\n",
    "                        final_conv_length=29).create_network()\n",
    "to_dense_prediction_model(model)\n",
    "\n",
    "if cuda:\n",
    "    model.cuda()\n",
    "from braindecode.torch_ext.util import np_to_var\n",
    "import numpy as np\n",
    "# determine output size\n",
    "test_input = np_to_var(np.ones((2, in_chans, input_time_length, 1), dtype=np.float32))\n",
    "if cuda:\n",
    "    test_input = test_input.cuda()\n",
    "out = model(test_input)\n",
    "n_preds_per_input = out.cpu().data.numpy().shape[2]\n",
    "print(\"{:d} predictions per input/trial\".format(n_preds_per_input))\n",
    "n_receptive_field = input_time_length - n_preds_per_input\n",
    "receptive_field_ms = n_receptive_field * 1000.0 / train_cnts[0].info['sfreq']\n",
    "print(\"Receptive field: {:d}/{:.2f} (samples/ms)\".format(n_receptive_field,\n",
    "                                                      receptive_field_ms))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Create SignalAndTarget Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-07-05 13:10:21,878 WARNING : No end marker for start marker code 1 at sample 150051 found.\n",
      "2017-07-05 13:10:21,879 INFO : Trial per class:\n",
      "Counter({'Right Hand': 29, 'Words': 21, 'Feet': 19, 'Break': 18, 'Rotation': 4})\n",
      "2017-07-05 13:10:21,883 WARNING : No end marker for start marker code 4 at sample 152701 found.\n",
      "2017-07-05 13:10:21,884 INFO : Trial per class:\n",
      "Counter({'Feet': 31, 'Words': 26, 'Break': 20, 'Right Hand': 18, 'Rotation': 6})\n",
      "2017-07-05 13:10:21,887 WARNING : No end marker for start marker code 1 at sample 179301 found.\n",
      "2017-07-05 13:10:21,888 INFO : Trial per class:\n",
      "Counter({'Feet': 38, 'Words': 29, 'Break': 23, 'Right Hand': 22, 'Rotation': 7})\n"
     ]
    }
   ],
   "source": [
    "train_sets = [create_signal_target_from_raw_mne(cnt, name_to_code_with_breaks, [-receptive_field_ms,0], \n",
    "                                         name_to_stop_code_with_breaks) for cnt in train_cnts]\n",
    "train_set = concatenate_sets(train_sets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-07-05 13:10:21,909 WARNING : No end marker for start marker code 4 at sample 150601 found.\n",
      "2017-07-05 13:10:21,910 INFO : Trial per class:\n",
      "Counter({'Feet': 24, 'Right Hand': 24, 'Words': 19, 'Break': 19, 'Rotation': 10})\n",
      "2017-07-05 13:10:21,914 WARNING : No end marker for start marker code 4 at sample 149601 found.\n",
      "2017-07-05 13:10:21,915 INFO : Trial per class:\n",
      "Counter({'Feet': 30, 'Right Hand': 22, 'Words': 21, 'Break': 20, 'Rotation': 8})\n"
     ]
    }
   ],
   "source": [
    "test_sets = [create_signal_target_from_raw_mne(cnt, name_to_code_with_breaks, [-receptive_field_ms,0], \n",
    "                                         name_to_stop_code_with_breaks) for cnt in test_cnts]\n",
    "test_set = concatenate_sets(test_sets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from braindecode.datautil.splitters import split_into_two_sets\n",
    "\n",
    "train_set, valid_set = split_into_two_sets(train_set, first_set_fraction=0.8)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Setup optimizer and iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from braindecode.datautil.iterators import CropsFromTrialsIterator\n",
    "iterator = CropsFromTrialsIterator(batch_size=32,input_time_length=input_time_length,\n",
    "                                  n_preds_per_input=n_preds_per_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Setup Monitors, Loss function, Stop Criteria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from braindecode.experiments.experiment import Experiment\n",
    "from braindecode.experiments.monitors import RuntimeMonitor, LossMonitor, CroppedTrialMisclassMonitor, MisclassMonitor\n",
    "from braindecode.experiments.stopcriteria import MaxEpochs\n",
    "import torch.nn.functional as F\n",
    "import torch as th\n",
    "from braindecode.torch_ext.modules import Expression\n",
    "\n",
    "\n",
    "loss_function = lambda preds, targets: F.nll_loss(th.mean(preds, dim=2)[:,:,0], targets)\n",
    "\n",
    "model_constraint = None\n",
    "monitors = [LossMonitor(), MisclassMonitor(col_suffix='sample_misclass'),\n",
    "            CroppedTrialMisclassMonitor(input_time_length), RuntimeMonitor(),]\n",
    "stop_criterion = MaxEpochs(20)\n",
    "exp = Experiment(model, train_set, valid_set, test_set, iterator, loss_function, optimizer, model_constraint,\n",
    "          monitors, stop_criterion, remember_best_column='valid_misclass',\n",
    "          run_after_early_stop=True, batch_modifier=None, cuda=cuda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Run experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-07-05 13:10:22,016 INFO : Run until first stop...\n",
      "2017-07-05 13:10:22,601 INFO : Epoch 0\n",
      "2017-07-05 13:10:22,602 INFO : train_loss                11.53333\n",
      "2017-07-05 13:10:22,602 INFO : valid_loss                12.37084\n",
      "2017-07-05 13:10:22,603 INFO : test_loss                 11.33717\n",
      "2017-07-05 13:10:22,604 INFO : train_sample_misclass     0.93548\n",
      "2017-07-05 13:10:22,604 INFO : valid_sample_misclass     0.98502\n",
      "2017-07-05 13:10:22,605 INFO : test_sample_misclass      0.91691\n",
      "2017-07-05 13:10:22,606 INFO : train_misclass            0.93976\n",
      "2017-07-05 13:10:22,606 INFO : valid_misclass            0.96774\n",
      "2017-07-05 13:10:22,607 INFO : test_misclass             0.90863\n",
      "2017-07-05 13:10:22,608 INFO : runtime                   0.00000\n",
      "2017-07-05 13:10:22,608 INFO : \n",
      "2017-07-05 13:10:22,610 INFO : New best valid_misclass: 0.967742\n",
      "2017-07-05 13:10:22,610 INFO : \n",
      "2017-07-05 13:10:23,656 INFO : Epoch 1\n",
      "2017-07-05 13:10:23,657 INFO : train_loss                1.50414\n",
      "2017-07-05 13:10:23,658 INFO : valid_loss                1.45471\n",
      "2017-07-05 13:10:23,659 INFO : test_loss                 1.55881\n",
      "2017-07-05 13:10:23,659 INFO : train_sample_misclass     0.64477\n",
      "2017-07-05 13:10:23,660 INFO : valid_sample_misclass     0.62696\n",
      "2017-07-05 13:10:23,661 INFO : test_sample_misclass      0.66721\n",
      "2017-07-05 13:10:23,661 INFO : train_misclass            0.58635\n",
      "2017-07-05 13:10:23,662 INFO : valid_misclass            0.59677\n",
      "2017-07-05 13:10:23,662 INFO : test_misclass             0.62437\n",
      "2017-07-05 13:10:23,663 INFO : runtime                   1.09606\n",
      "2017-07-05 13:10:23,664 INFO : \n",
      "2017-07-05 13:10:23,665 INFO : New best valid_misclass: 0.596774\n",
      "2017-07-05 13:10:23,666 INFO : \n",
      "2017-07-05 13:10:24,707 INFO : Epoch 2\n",
      "2017-07-05 13:10:24,708 INFO : train_loss                1.41926\n",
      "2017-07-05 13:10:24,709 INFO : valid_loss                1.48257\n",
      "2017-07-05 13:10:24,710 INFO : test_loss                 1.52955\n",
      "2017-07-05 13:10:24,710 INFO : train_sample_misclass     0.60706\n",
      "2017-07-05 13:10:24,711 INFO : valid_sample_misclass     0.62259\n",
      "2017-07-05 13:10:24,711 INFO : test_sample_misclass      0.64398\n",
      "2017-07-05 13:10:24,712 INFO : train_misclass            0.55823\n",
      "2017-07-05 13:10:24,713 INFO : valid_misclass            0.62903\n",
      "2017-07-05 13:10:24,713 INFO : test_misclass             0.64467\n",
      "2017-07-05 13:10:24,714 INFO : runtime                   1.05009\n",
      "2017-07-05 13:10:24,715 INFO : \n",
      "2017-07-05 13:10:25,753 INFO : Epoch 3\n",
      "2017-07-05 13:10:25,754 INFO : train_loss                1.42563\n",
      "2017-07-05 13:10:25,755 INFO : valid_loss                1.49839\n",
      "2017-07-05 13:10:25,756 INFO : test_loss                 1.61948\n",
      "2017-07-05 13:10:25,756 INFO : train_sample_misclass     0.61650\n",
      "2017-07-05 13:10:25,757 INFO : valid_sample_misclass     0.65266\n",
      "2017-07-05 13:10:25,758 INFO : test_sample_misclass      0.67657\n",
      "2017-07-05 13:10:25,758 INFO : train_misclass            0.60241\n",
      "2017-07-05 13:10:25,759 INFO : valid_misclass            0.72581\n",
      "2017-07-05 13:10:25,759 INFO : test_misclass             0.67513\n",
      "2017-07-05 13:10:25,760 INFO : runtime                   1.04545\n",
      "2017-07-05 13:10:25,761 INFO : \n",
      "2017-07-05 13:10:26,797 INFO : Epoch 4\n",
      "2017-07-05 13:10:26,798 INFO : train_loss                1.38610\n",
      "2017-07-05 13:10:26,799 INFO : valid_loss                1.48648\n",
      "2017-07-05 13:10:26,799 INFO : test_loss                 1.55085\n",
      "2017-07-05 13:10:26,800 INFO : train_sample_misclass     0.59853\n",
      "2017-07-05 13:10:26,800 INFO : valid_sample_misclass     0.64496\n",
      "2017-07-05 13:10:26,801 INFO : test_sample_misclass      0.65656\n",
      "2017-07-05 13:10:26,802 INFO : train_misclass            0.52610\n",
      "2017-07-05 13:10:26,802 INFO : valid_misclass            0.67742\n",
      "2017-07-05 13:10:26,803 INFO : test_misclass             0.62944\n",
      "2017-07-05 13:10:26,804 INFO : runtime                   1.04518\n",
      "2017-07-05 13:10:26,804 INFO : \n",
      "2017-07-05 13:10:27,841 INFO : Epoch 5\n",
      "2017-07-05 13:10:27,842 INFO : train_loss                1.41484\n",
      "2017-07-05 13:10:27,843 INFO : valid_loss                1.44856\n",
      "2017-07-05 13:10:27,844 INFO : test_loss                 1.55547\n",
      "2017-07-05 13:10:27,844 INFO : train_sample_misclass     0.60230\n",
      "2017-07-05 13:10:27,845 INFO : valid_sample_misclass     0.62832\n",
      "2017-07-05 13:10:27,845 INFO : test_sample_misclass      0.65074\n",
      "2017-07-05 13:10:27,846 INFO : train_misclass            0.51406\n",
      "2017-07-05 13:10:27,847 INFO : valid_misclass            0.58065\n",
      "2017-07-05 13:10:27,847 INFO : test_misclass             0.63452\n",
      "2017-07-05 13:10:27,848 INFO : runtime                   1.04737\n",
      "2017-07-05 13:10:27,849 INFO : \n",
      "2017-07-05 13:10:27,850 INFO : New best valid_misclass: 0.580645\n",
      "2017-07-05 13:10:27,851 INFO : \n",
      "2017-07-05 13:10:28,890 INFO : Epoch 6\n",
      "2017-07-05 13:10:28,891 INFO : train_loss                1.36760\n",
      "2017-07-05 13:10:28,892 INFO : valid_loss                1.41204\n",
      "2017-07-05 13:10:28,893 INFO : test_loss                 1.52590\n",
      "2017-07-05 13:10:28,893 INFO : train_sample_misclass     0.58988\n",
      "2017-07-05 13:10:28,894 INFO : valid_sample_misclass     0.63861\n",
      "2017-07-05 13:10:28,895 INFO : test_sample_misclass      0.64623\n",
      "2017-07-05 13:10:28,895 INFO : train_misclass            0.49799\n",
      "2017-07-05 13:10:28,896 INFO : valid_misclass            0.61290\n",
      "2017-07-05 13:10:28,896 INFO : test_misclass             0.60406\n",
      "2017-07-05 13:10:28,897 INFO : runtime                   1.04936\n",
      "2017-07-05 13:10:28,898 INFO : \n",
      "2017-07-05 13:10:29,933 INFO : Epoch 7\n",
      "2017-07-05 13:10:29,934 INFO : train_loss                1.37728\n",
      "2017-07-05 13:10:29,934 INFO : valid_loss                1.43272\n",
      "2017-07-05 13:10:29,935 INFO : test_loss                 1.50316\n",
      "2017-07-05 13:10:29,936 INFO : train_sample_misclass     0.57462\n",
      "2017-07-05 13:10:29,936 INFO : valid_sample_misclass     0.62801\n",
      "2017-07-05 13:10:29,937 INFO : test_sample_misclass      0.63327\n",
      "2017-07-05 13:10:29,938 INFO : train_misclass            0.51807\n",
      "2017-07-05 13:10:29,938 INFO : valid_misclass            0.70968\n",
      "2017-07-05 13:10:29,939 INFO : test_misclass             0.63452\n",
      "2017-07-05 13:10:29,939 INFO : runtime                   1.04274\n",
      "2017-07-05 13:10:29,940 INFO : \n",
      "2017-07-05 13:10:30,979 INFO : Epoch 8\n",
      "2017-07-05 13:10:30,980 INFO : train_loss                1.37169\n",
      "2017-07-05 13:10:30,981 INFO : valid_loss                1.42813\n",
      "2017-07-05 13:10:30,982 INFO : test_loss                 1.53069\n",
      "2017-07-05 13:10:30,982 INFO : train_sample_misclass     0.58574\n",
      "2017-07-05 13:10:30,983 INFO : valid_sample_misclass     0.65559\n",
      "2017-07-05 13:10:30,983 INFO : test_sample_misclass      0.67227\n",
      "2017-07-05 13:10:30,984 INFO : train_misclass            0.55823\n",
      "2017-07-05 13:10:30,985 INFO : valid_misclass            0.67742\n",
      "2017-07-05 13:10:30,985 INFO : test_misclass             0.65990\n",
      "2017-07-05 13:10:30,986 INFO : runtime                   1.04512\n",
      "2017-07-05 13:10:30,987 INFO : \n",
      "2017-07-05 13:10:32,024 INFO : Epoch 9\n",
      "2017-07-05 13:10:32,025 INFO : train_loss                1.36508\n",
      "2017-07-05 13:10:32,026 INFO : valid_loss                1.43018\n",
      "2017-07-05 13:10:32,026 INFO : test_loss                 1.53601\n",
      "2017-07-05 13:10:32,027 INFO : train_sample_misclass     0.58852\n",
      "2017-07-05 13:10:32,028 INFO : valid_sample_misclass     0.65428\n",
      "2017-07-05 13:10:32,028 INFO : test_sample_misclass      0.66769\n",
      "2017-07-05 13:10:32,029 INFO : train_misclass            0.57028\n",
      "2017-07-05 13:10:32,029 INFO : valid_misclass            0.67742\n",
      "2017-07-05 13:10:32,030 INFO : test_misclass             0.65482\n",
      "2017-07-05 13:10:32,031 INFO : runtime                   1.04509\n",
      "2017-07-05 13:10:32,031 INFO : \n",
      "2017-07-05 13:10:33,074 INFO : Epoch 10\n",
      "2017-07-05 13:10:33,074 INFO : train_loss                1.34019\n",
      "2017-07-05 13:10:33,075 INFO : valid_loss                1.47361\n",
      "2017-07-05 13:10:33,076 INFO : test_loss                 1.52654\n",
      "2017-07-05 13:10:33,076 INFO : train_sample_misclass     0.57780\n",
      "2017-07-05 13:10:33,077 INFO : valid_sample_misclass     0.64126\n",
      "2017-07-05 13:10:33,078 INFO : test_sample_misclass      0.65008\n",
      "2017-07-05 13:10:33,078 INFO : train_misclass            0.48594\n",
      "2017-07-05 13:10:33,079 INFO : valid_misclass            0.56452\n",
      "2017-07-05 13:10:33,079 INFO : test_misclass             0.60406\n",
      "2017-07-05 13:10:33,080 INFO : runtime                   1.04898\n",
      "2017-07-05 13:10:33,081 INFO : \n",
      "2017-07-05 13:10:33,082 INFO : New best valid_misclass: 0.564516\n",
      "2017-07-05 13:10:33,083 INFO : \n",
      "2017-07-05 13:10:34,122 INFO : Epoch 11\n",
      "2017-07-05 13:10:34,123 INFO : train_loss                1.37248\n",
      "2017-07-05 13:10:34,123 INFO : valid_loss                1.39706\n",
      "2017-07-05 13:10:34,124 INFO : test_loss                 1.57353\n",
      "2017-07-05 13:10:34,124 INFO : train_sample_misclass     0.58344\n",
      "2017-07-05 13:10:34,125 INFO : valid_sample_misclass     0.64056\n",
      "2017-07-05 13:10:34,126 INFO : test_sample_misclass      0.67993\n",
      "2017-07-05 13:10:34,126 INFO : train_misclass            0.51406\n",
      "2017-07-05 13:10:34,127 INFO : valid_misclass            0.64516\n",
      "2017-07-05 13:10:34,128 INFO : test_misclass             0.65482\n",
      "2017-07-05 13:10:34,128 INFO : runtime                   1.04916\n",
      "2017-07-05 13:10:34,129 INFO : \n",
      "2017-07-05 13:10:35,166 INFO : Epoch 12\n",
      "2017-07-05 13:10:35,167 INFO : train_loss                1.38699\n",
      "2017-07-05 13:10:35,168 INFO : valid_loss                1.48617\n",
      "2017-07-05 13:10:35,168 INFO : test_loss                 1.57766\n",
      "2017-07-05 13:10:35,169 INFO : train_sample_misclass     0.60407\n",
      "2017-07-05 13:10:35,170 INFO : valid_sample_misclass     0.62667\n",
      "2017-07-05 13:10:35,170 INFO : test_sample_misclass      0.66531\n",
      "2017-07-05 13:10:35,171 INFO : train_misclass            0.56627\n",
      "2017-07-05 13:10:35,172 INFO : valid_misclass            0.61290\n",
      "2017-07-05 13:10:35,172 INFO : test_misclass             0.66497\n",
      "2017-07-05 13:10:35,173 INFO : runtime                   1.04371\n",
      "2017-07-05 13:10:35,173 INFO : \n",
      "2017-07-05 13:10:36,210 INFO : Epoch 13\n",
      "2017-07-05 13:10:36,211 INFO : train_loss                1.34750\n",
      "2017-07-05 13:10:36,211 INFO : valid_loss                1.47149\n",
      "2017-07-05 13:10:36,212 INFO : test_loss                 1.57028\n",
      "2017-07-05 13:10:36,213 INFO : train_sample_misclass     0.56923\n",
      "2017-07-05 13:10:36,213 INFO : valid_sample_misclass     0.63300\n",
      "2017-07-05 13:10:36,214 INFO : test_sample_misclass      0.65084\n",
      "2017-07-05 13:10:36,215 INFO : train_misclass            0.50602\n",
      "2017-07-05 13:10:36,215 INFO : valid_misclass            0.56452\n",
      "2017-07-05 13:10:36,216 INFO : test_misclass             0.59391\n",
      "2017-07-05 13:10:36,216 INFO : runtime                   1.04310\n",
      "2017-07-05 13:10:36,217 INFO : \n",
      "2017-07-05 13:10:36,219 INFO : New best valid_misclass: 0.564516\n",
      "2017-07-05 13:10:36,219 INFO : \n",
      "2017-07-05 13:10:37,261 INFO : Epoch 14\n",
      "2017-07-05 13:10:37,262 INFO : train_loss                1.35358\n",
      "2017-07-05 13:10:37,263 INFO : valid_loss                1.46056\n",
      "2017-07-05 13:10:37,263 INFO : test_loss                 1.54272\n",
      "2017-07-05 13:10:37,264 INFO : train_sample_misclass     0.58878\n",
      "2017-07-05 13:10:37,265 INFO : valid_sample_misclass     0.63554\n",
      "2017-07-05 13:10:37,265 INFO : test_sample_misclass      0.65247\n",
      "2017-07-05 13:10:37,266 INFO : train_misclass            0.48996\n",
      "2017-07-05 13:10:37,266 INFO : valid_misclass            0.59677\n",
      "2017-07-05 13:10:37,267 INFO : test_misclass             0.61421\n",
      "2017-07-05 13:10:37,268 INFO : runtime                   1.05266\n",
      "2017-07-05 13:10:37,268 INFO : \n",
      "2017-07-05 13:10:38,311 INFO : Epoch 15\n",
      "2017-07-05 13:10:38,312 INFO : train_loss                1.32901\n",
      "2017-07-05 13:10:38,313 INFO : valid_loss                1.38894\n",
      "2017-07-05 13:10:38,313 INFO : test_loss                 1.51226\n",
      "2017-07-05 13:10:38,314 INFO : train_sample_misclass     0.56179\n",
      "2017-07-05 13:10:38,315 INFO : valid_sample_misclass     0.62443\n",
      "2017-07-05 13:10:38,315 INFO : test_sample_misclass      0.64149\n",
      "2017-07-05 13:10:38,316 INFO : train_misclass            0.45783\n",
      "2017-07-05 13:10:38,316 INFO : valid_misclass            0.56452\n",
      "2017-07-05 13:10:38,317 INFO : test_misclass             0.60914\n",
      "2017-07-05 13:10:38,318 INFO : runtime                   1.04960\n",
      "2017-07-05 13:10:38,318 INFO : \n",
      "2017-07-05 13:10:38,320 INFO : New best valid_misclass: 0.564516\n",
      "2017-07-05 13:10:38,320 INFO : \n",
      "2017-07-05 13:10:39,367 INFO : Epoch 16\n",
      "2017-07-05 13:10:39,368 INFO : train_loss                1.35494\n",
      "2017-07-05 13:10:39,368 INFO : valid_loss                1.46781\n",
      "2017-07-05 13:10:39,369 INFO : test_loss                 1.55805\n",
      "2017-07-05 13:10:39,370 INFO : train_sample_misclass     0.58213\n",
      "2017-07-05 13:10:39,370 INFO : valid_sample_misclass     0.61860\n",
      "2017-07-05 13:10:39,371 INFO : test_sample_misclass      0.64870\n",
      "2017-07-05 13:10:39,371 INFO : train_misclass            0.55020\n",
      "2017-07-05 13:10:39,372 INFO : valid_misclass            0.61290\n",
      "2017-07-05 13:10:39,373 INFO : test_misclass             0.65990\n",
      "2017-07-05 13:10:39,373 INFO : runtime                   1.05210\n",
      "2017-07-05 13:10:39,374 INFO : \n",
      "2017-07-05 13:10:40,413 INFO : Epoch 17\n",
      "2017-07-05 13:10:40,414 INFO : train_loss                1.33417\n",
      "2017-07-05 13:10:40,415 INFO : valid_loss                1.43405\n",
      "2017-07-05 13:10:40,415 INFO : test_loss                 1.50453\n",
      "2017-07-05 13:10:40,416 INFO : train_sample_misclass     0.57871\n",
      "2017-07-05 13:10:40,417 INFO : valid_sample_misclass     0.63641\n",
      "2017-07-05 13:10:40,417 INFO : test_sample_misclass      0.64038\n",
      "2017-07-05 13:10:40,418 INFO : train_misclass            0.51004\n",
      "2017-07-05 13:10:40,418 INFO : valid_misclass            0.56452\n",
      "2017-07-05 13:10:40,419 INFO : test_misclass             0.63452\n",
      "2017-07-05 13:10:40,420 INFO : runtime                   1.04953\n",
      "2017-07-05 13:10:40,420 INFO : \n",
      "2017-07-05 13:10:40,422 INFO : New best valid_misclass: 0.564516\n",
      "2017-07-05 13:10:40,423 INFO : \n",
      "2017-07-05 13:10:41,462 INFO : Epoch 18\n",
      "2017-07-05 13:10:41,463 INFO : train_loss                1.37133\n",
      "2017-07-05 13:10:41,464 INFO : valid_loss                1.48794\n",
      "2017-07-05 13:10:41,465 INFO : test_loss                 1.61320\n",
      "2017-07-05 13:10:41,465 INFO : train_sample_misclass     0.59854\n",
      "2017-07-05 13:10:41,466 INFO : valid_sample_misclass     0.63155\n",
      "2017-07-05 13:10:41,466 INFO : test_sample_misclass      0.66630\n",
      "2017-07-05 13:10:41,467 INFO : train_misclass            0.53414\n",
      "2017-07-05 13:10:41,468 INFO : valid_misclass            0.54839\n",
      "2017-07-05 13:10:41,468 INFO : test_misclass             0.63959\n",
      "2017-07-05 13:10:41,469 INFO : runtime                   1.04950\n",
      "2017-07-05 13:10:41,470 INFO : \n",
      "2017-07-05 13:10:41,471 INFO : New best valid_misclass: 0.548387\n",
      "2017-07-05 13:10:41,472 INFO : \n",
      "2017-07-05 13:10:42,514 INFO : Epoch 19\n",
      "2017-07-05 13:10:42,515 INFO : train_loss                1.38336\n",
      "2017-07-05 13:10:42,516 INFO : valid_loss                1.45849\n",
      "2017-07-05 13:10:42,516 INFO : test_loss                 1.53109\n",
      "2017-07-05 13:10:42,517 INFO : train_sample_misclass     0.58641\n",
      "2017-07-05 13:10:42,517 INFO : valid_sample_misclass     0.62703\n",
      "2017-07-05 13:10:42,518 INFO : test_sample_misclass      0.64048\n",
      "2017-07-05 13:10:42,519 INFO : train_misclass            0.50201\n",
      "2017-07-05 13:10:42,519 INFO : valid_misclass            0.69355\n",
      "2017-07-05 13:10:42,520 INFO : test_misclass             0.59391\n",
      "2017-07-05 13:10:42,521 INFO : runtime                   1.05205\n",
      "2017-07-05 13:10:42,521 INFO : \n",
      "2017-07-05 13:10:43,566 INFO : Epoch 20\n",
      "2017-07-05 13:10:43,567 INFO : train_loss                1.33203\n",
      "2017-07-05 13:10:43,568 INFO : valid_loss                1.36307\n",
      "2017-07-05 13:10:43,569 INFO : test_loss                 1.50656\n",
      "2017-07-05 13:10:43,569 INFO : train_sample_misclass     0.56715\n",
      "2017-07-05 13:10:43,570 INFO : valid_sample_misclass     0.60826\n",
      "2017-07-05 13:10:43,570 INFO : test_sample_misclass      0.63761\n",
      "2017-07-05 13:10:43,571 INFO : train_misclass            0.47390\n",
      "2017-07-05 13:10:43,572 INFO : valid_misclass            0.61290\n",
      "2017-07-05 13:10:43,572 INFO : test_misclass             0.58883\n",
      "2017-07-05 13:10:43,573 INFO : runtime                   1.05095\n",
      "2017-07-05 13:10:43,574 INFO : \n",
      "2017-07-05 13:10:43,574 INFO : Setup for second stop...\n",
      "2017-07-05 13:10:43,577 INFO : Train loss to reach 1.37133\n",
      "2017-07-05 13:10:43,578 INFO : Run until second stop...\n",
      "2017-07-05 13:10:44,198 INFO : Epoch 19\n",
      "2017-07-05 13:10:44,199 INFO : train_loss                1.39385\n",
      "2017-07-05 13:10:44,199 INFO : valid_loss                1.48794\n",
      "2017-07-05 13:10:44,200 INFO : test_loss                 1.61320\n",
      "2017-07-05 13:10:44,200 INFO : train_sample_misclass     0.60491\n",
      "2017-07-05 13:10:44,201 INFO : valid_sample_misclass     0.63155\n",
      "2017-07-05 13:10:44,202 INFO : test_sample_misclass      0.66630\n",
      "2017-07-05 13:10:44,202 INFO : train_misclass            0.53698\n",
      "2017-07-05 13:10:44,203 INFO : valid_misclass            0.54839\n",
      "2017-07-05 13:10:44,204 INFO : test_misclass             0.63959\n",
      "2017-07-05 13:10:44,204 INFO : runtime                   0.55378\n",
      "2017-07-05 13:10:44,205 INFO : \n",
      "2017-07-05 13:10:45,423 INFO : Epoch 20\n",
      "2017-07-05 13:10:45,424 INFO : train_loss                1.35231\n",
      "2017-07-05 13:10:45,425 INFO : valid_loss                1.33071\n",
      "2017-07-05 13:10:45,426 INFO : test_loss                 1.58230\n",
      "2017-07-05 13:10:45,426 INFO : train_sample_misclass     0.57709\n",
      "2017-07-05 13:10:45,427 INFO : valid_sample_misclass     0.62023\n",
      "2017-07-05 13:10:45,428 INFO : test_sample_misclass      0.69375\n",
      "2017-07-05 13:10:45,428 INFO : train_misclass            0.48553\n",
      "2017-07-05 13:10:45,429 INFO : valid_misclass            0.54839\n",
      "2017-07-05 13:10:45,430 INFO : test_misclass             0.69036\n",
      "2017-07-05 13:10:45,430 INFO : runtime                   1.23906\n",
      "2017-07-05 13:10:45,431 INFO : \n"
     ]
    }
   ],
   "source": [
    "exp.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We arrive only at 31% accuracy. With only 3 sensors and 3 training runs, cannot expect too much great performance :)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
