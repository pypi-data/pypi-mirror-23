#!/usr/bin/python

"""

Copyright (C) 2016-2017 GradientOne Inc. - All Rights Reserved
Unauthorized copying or distribution of this file is strictly prohibited
without the express permission of GradientOne Inc.

"""


import collections
import csv
import gzip
import json
import os
import requests
import traceback
import zlib
from configparser import ConfigParser
from datetime import datetime
from requests_toolbelt.multipart.encoder import MultipartEncoder

import gateway_helpers
from gateway_helpers import logger, get_headers
from base import BaseClient
from scope import ScopeClient

from gradientone.transformers import Transformer

cfg = ConfigParser()
cfg.read('/etc/gradient_one.cfg')
COMMON_SETTINGS = cfg['common']
CLIENT_SETTINGS = cfg['client']
try:
    TMPDIR = CLIENT_SETTINGS["TMPDIR"]
except:
    TMPDIR = "/tmp"

SENTRY_CLIENT = gateway_helpers.get_sentry()

CLIENT_ID = 'ORIGINAL'
SAMPLE_FILE = 'MDO30121M.txt'
MAX_VALID_MEAS_VAL = 1e36

if COMMON_SETTINGS["DOMAIN"].find("localhost") == 0 or COMMON_SETTINGS["DOMAIN"].find("127.0.0.1") == 0:  # noqa
    BASE_URL = "http://" + COMMON_SETTINGS["DOMAIN"]
else:
    BASE_URL = "https://" + COMMON_SETTINGS["DOMAIN"]

CONFIG_URL = (BASE_URL + "/testplansummary/" +
              COMMON_SETTINGS['COMPANYNAME'] + '/' +
              COMMON_SETTINGS['HARDWARENAME'])


class Transmitter(BaseClient):
    # Transmits trace data received from instrument up to the server

    def __init__(self, command_id="unknown_id"):
        self.session = requests.session()
        self.session.headers.update(gateway_helpers.get_headers())
        trace_dict = {
            "test_run_id": command_id,
            "command_id": command_id,
        }
        self.trace_dict = collections.defaultdict(str, trace_dict)
        self.test_run_id = command_id
        self.command_id = command_id
        self.results_dict = collections.defaultdict(str)
        self.dec_factor = 1
        self.upload_polling_metadata = True

    def format_csv(self):
        pass

    def post_result(self):
        if "test_results" in self.results_dict.keys():
            del self.results_dict["test_results"]
        if "blob_key" not in self.results_dict.keys():
            self.results_dict["blob_key"] = self.blob_key

        PAYLOAD = {
            # 'result' is for the datastore entity
            'result': {
                'id': self.results_dict['test_run_id'],
                'command_id': self.results_dict['test_run_id'],
                'config': self.results_dict['config_name'],
                'instrument_type': self.results_dict['instrument_type'],
                'tags': self.results_dict['tags'],
                'plan': self.results_dict['plan'],
                'info': self.results_dict,  # ToDo: remove unnecessary values
                'dut': self.results_dict['dut'],
            },
            'fields': [
                {
                    'name': 'config_name',
                    'value': self.results_dict['config_name'],
                    'type': 'text'
                },
                {
                        'name': 'hardware_name',
                        'value': self.results_dict['hardware_name'],
                        'type': 'text'
                },
                {
                        'name': 'instrument_type',
                        'value': self.results_dict['instrument_type'],
                        'type': 'text'
                },
                {
                        'name': 'test_run_id',
                        'value': self.results_dict['test_run_id'],
                        'type': 'text'
                },
                {
                        'name': 'start_datetime',
                        'value': str(datetime.now()),
                        'type': 'text'
                },
            ],
            'index_name': 'results'
        }
        url = BASE_URL + "/results"
        self.post(url, data=json.dumps(PAYLOAD))

    def generate_results_dict(self):
        self.results_dict['config_name'] = "unknown"

    def post_wrapper(self, url, data="", headers={}):
        """
        This is a wrapper function to the sessions.post function to capture
        errors in the log.
        :param url: the url that the post is going to
        :param data: the data. Should be a string, if it is a dict it will be
        serialized to JSON
        :param headers: the headers. If empty, will be set to the default
        value in get_headers
        """
        if not headers:
            headers = gateway_helpers.get_headers()
        if isinstance(data, dict):
            data = json.dumps(data)
        response = self.post(url, data=data, headers=headers)
        assert response.status_code == 200
        logger.debug(url + " response.reason= %s" % response.reason)
        logger.debug(url + " response.status_code=%s" % response.status_code)
        return response

    def test_complete(self):
        """transmit test complete function sends a json object that is used
           to update DB on test status. Primarily metadata and indexing.
           No waveforms are sent here.
        """
        logger.info("Posting results")
        self.transmit_result_blob()
        self.generate_results_dict()
        data = json.dumps(self.results_dict)
        if self.results_dict['config_name'] == 'I2C':
            # trigger the appropriate analysis handler
            _response = self.get(BASE_URL + "/analysis",
                                 params={"test_run_id": self.test_run_id,
                                         "suite": "skywave"})
            expected_result = "[\"/markers?test_run_id=" + str(
                self.test_run_id) + "\", \"/results/" + str(
                self.test_run_id) + "/metadata\"]"
            assert _response.status_code == 200
            assert _response.text == expected_result
        if self.upload_polling_metadata:
            polling_metadata_url = (BASE_URL + "/polling/results/metadata/" +
                                    str(self.test_run_id))
            self.post(polling_metadata_url, data=data)
        data = {'command_id': self.test_run_id, 'status': 'complete'}
        commands_url = BASE_URL + "/commands"
        response = self.put(commands_url, data=json.dumps(data),
                            headers=get_headers())
        assert response.status_code == 200
        self.post_result()

    def transmit_result_blob(self, infile=''):
        """Sends test results to the blobstore

        End to end waveforms are sent here.
        """
        if not infile:
            filename = 'full-trace-%s.json' % self.test_run_id
            infile = os.path.join(TMPDIR, filename)
        logger.debug("transmitting blob")
        gzip_file = infile + '.gz'
        config_name = self.trace_dict['config_name']
        with open(infile) as f_in, gzip.open(gzip_file, 'wb') as f_out:
            f_out.writelines(f_in)
        data_type = 'application/x-gzip'

        if self.trace_dict["test_run_id"] == "unknown_id":
            raise ValueError("can't upload with unknown_id")
        blobfile = str(config_name) + ':' + str(self.trace_dict['test_run_id'])
        multipartblob = MultipartEncoder(
            fields={
                'file': (blobfile, open(gzip_file, 'rb'), data_type),
                'test_run_id': str(self.trace_dict['test_run_id']),
                'file_key': gzip_file.split('/')[-1],
            }
        )
        blob_url = self.get(BASE_URL + "/upload/geturl")
        response = self.post(
            blob_url.text,
            data=multipartblob,
            headers={
                'Content-Type': multipartblob.content_type
            })

        self.blob_key = response.text

    def transmit_trace(self):
        """results transmission for generic instruments"""
        self.session = requests.session()
        self.session.headers.update(gateway_helpers.get_headers())
        try:
            logger.debug("transmitting trace")
            # complete transmission indexing blobstore data
            self.test_complete()
        except Exception:
            logger.error("Exception occurred during trace transmission")
            logger.error(traceback.format_exc())
            SENTRY_CLIENT.captureException()
        finally:
            tid = self.trace_dict['test_run_id']
            self.post_logfile(test_run_id=tid)


class CANOpenTransmitter(Transmitter):
    """
    Transmits data for CANOpen devices
    """

    def __init__(self, data=None, units=None,
                 config={"arg": "unknown_config", "id": "unknown_id"}):

        Transmitter.__init__(self, config["id"])
        # the data is a list of dictionaries, where each key is a column
        # header, and the value with that key is the row value at that column,
        # i.e., [{'y': 0, 'x1': 9.9, 'x2': 3.2},
        #        {'y': 1, 'x1': 9.8, 'x2': 3.1},]
        self.data = data
        # the units is a dictionary with the unit names of each column in the
        # data, i.e., {'y': 's', 'x1': 'm', 'x2': 'km'}
        self.units = units
        self.config = config
        if "id" in config.keys():
            self.test_run_id = config["id"]

        # do not upload any metadata
        self.upload_polling_metadata = False
        self.transformer = Transformer(None)
        # repackage the data into a dict
        for point in self.data:
            for key in point.keys():
                if key not in self.trace_dict.keys():
                    self.trace_dict[key] = [point[key]]
                else:
                    self.trace_dict[key].append(point[key])
        self.transformer.trace_dict = self.trace_dict
        self.transformer.test_run_id = self.test_run_id
        self.transformer._write_trace_dict()

    def generate_results_dict(self):
        if self.config is not None:
            self.results_dict["config_name"] = self.config["arg"]
        else:
            self.results_dict["config_name"] = "unknown"
        self.results_dict["hardware_name"] = COMMON_SETTINGS['HARDWARENAME']
        self.results_dict["company_nickname"] = COMMON_SETTINGS['COMPANYNAME']
        self.results_dict["instrument_type"] = "Motor"
        self.results_dict["test_run_id"] = self.test_run_id
        self.results_dict["more_metadata"] = {"units": self.units}

    def post_result(self):
        if "blob_key" not in self.results_dict.keys():
            self.results_dict["blob_key"] = self.blob_key
        PAYLOAD = {
            'result': {
                'id': self.results_dict['test_run_id'],
                'command_id': self.results_dict['test_run_id'],
                'config': self.results_dict['config_name'],
                'instrument_type': self.results_dict['instrument_type'],
                'tags': self.results_dict['tags'],
                'plan': self.results_dict['plan'],
                'info': self.results_dict,  # ToDo: remove unnecessary values
                'dut': self.results_dict['dut'],
            },
            "fields": [{
                "name": "config_name",
                "value": self.results_dict["config_name"],
                "type": "text"
            },
                {
                    "name": "hardware_name",
                    "value": self.results_dict["hardware_name"],
                    "type": "text"
                },
                {
                    "name": "instrument_type",
                    "value": self.results_dict["instrument_type"],
                    "type": "text"
                },
                {
                    "name": "test_run_id",
                    "value": self.results_dict["test_run_id"],
                    "type": "text"
                },
                {
                    "name": "parameters",
                    "value": ", ".join(self.config["info"]["config_excerpt"]["properties"]),  # noqa
                    "type": "text"
                },
                {
                    "name": "start_datetime",
                    "value": str(datetime.now()),
                    "type": "text"
                }
            ],
            "index_name": "results"
        }
        url = BASE_URL + "/results"
        self.post(url, data=json.dumps(PAYLOAD))


class ScopeTransmitter(Transmitter, ScopeClient):
    """transmits data to server for scope information"""

    def __init__(self, transformer=None, trace_data=None, session=None):
        Transmitter.__init__(self)
        self.test_run = collections.defaultdict(int)
        if transformer:
            self.test_run.update(transformer.test_run)
            self.test_run_id = self.test_run['id']
            self.command_id = self.test_run['id']
            self.time_step = transformer.time_step
            # the transformer ce_dict holds initial instructions
            self.ce_dict = transformer.ce_dict
            self.config_scorecard = transformer.config_scorecard
            self.g1_measurement_results = transformer.g1_measurement_results
            filename = 'summary-waveform-%s.json' % self.test_run_id
            summary_file = os.path.join(TMPDIR, filename)
            with open(summary_file, 'r') as f:
                self.summary_voltages = f.read()
                try:
                    self.summary_voltages = json.loads(self.summary_voltages)
                except Exception as e:
                    logger.warning(e, exc_info=True)
        else:
            self.ce_dict = {}
            self.time_step = 1.0
        self.trace_dict = collections.defaultdict(int)
        if trace_data:
            self.trace_dict.update(trace_data)

        self.blob_key = None
        self.raw_data_url = (
            BASE_URL + "/results/%s.json" % self.trace_dict['test_run_id']
        )

        # this is updated with the reported config_excerpt
        if "config_excerpt" in self.trace_dict.keys():
            self.ce_dict.update(self.trace_dict['config_excerpt'])
        self.config_scorecard = transformer.config_scorecard
        self.more_metadata = collections.defaultdict(str)

        if cfg.getboolean('client', 'SIMULATED'):
            self.ce_dict['enabled_list'] = ['ch1']
        # get the default channel
        self.results_dict = {
            'test_run_id': self.trace_dict['test_run_id'],
            'channel_headers': self.ce_dict['enabled_list'],
            'test_results': transformer.first_slice,
            'hardware_name': COMMON_SETTINGS['HARDWARENAME'],
            'company_nickname': COMMON_SETTINGS['COMPANYNAME'],
            'raw_data_url': self.raw_data_url,
            'start_tse': int(self.trace_dict['start_tse']),
            'test_plan': self.trace_dict['test_plan'],
            'config_name': self.trace_dict['config_name'],
            'record_length': self.trace_dict['record_length'],
            'time_per_record': self.ce_dict['acquisition']['time_per_record'],
            'time_step': self.time_step,
            # for the new Result model, we'd just need the following
            'command_id': '',
            'config': self.trace_dict['config_name'],
            'instrument_type': self.trace_dict['instrument_type'],
            'category': '',
            'tags': [],
            'plan': '',
            'dut': '',
            # 'info': {'metadata': {}},  # everything else should migrate
        }
        self.dec_factor = 1
        self.slices = []

    def format_csv(self):
        fileblob = open('tempfile.csv', 'w')
        cwriter = csv.writer(fileblob)
        cwriter.writerow(self.ce_dict['enabled_list'])
        voltage_lists = []
        if not self.summary_voltages:
            fileblob.close()
            return
        for ch in self.summary_voltages:
            voltage_lists.append(self.summary_voltages[ch])
        for row in zip(*voltage_lists):
            cwriter.writerow(row)
        fileblob.close()

    def generate_results_dict(self):
        chart_url = 'https://' + \
            COMMON_SETTINGS['DOMAIN'] + '/result/' + str(self.test_run_id)
        screenshot_url = (BASE_URL + '/blob?file_key=screenshot-' +
                          str(self.test_run_id))
        self.trace_dict['instr_info']['scope_screenshot_url'] = screenshot_url
        self.results_dict.update(self.trace_dict['instr_info'])
        extra_data = {
            'blob_key': self.blob_key,
            'results_link': chart_url,
            'test_results': self.raw_data_url,
            'raw_setup': self.trace_dict['raw_setup'],
            'config_input': self.ce_dict,
            'config_excerpt': self.trace_dict['config_excerpt'],
            'measurements': self.trace_dict['meas_dict'],
            'more_options': self.trace_dict['more_options'],
            'g1_measurement': self.trace_dict['g1_measurement'],
            'dec_factor': self.dec_factor,
            'config_scorecard': self.config_scorecard,
            'more_metadata': self.more_metadata,
            'g1_measurement_results': self.g1_measurement_results,
            'company_nickname': COMMON_SETTINGS['COMPANYNAME'],
            'instrument_type': self.trace_dict['instrument_type'],
        }
        self.results_dict.update(extra_data)

    def transmit_file(self, filepath, content_type='application/x-gzip'):
        """transmits file to blobstore

        - assumes the file is already gzipped
        - deletes the local file after successfully transmitting
        """
        test_run_id = str(self.trace_dict['test_run_id'])
        if not filepath:
            filepath = filename = test_run_id + "-file.gz"
        else:
            filename = filepath.split('/')[-1]
        multipartblob = MultipartEncoder(
            fields={
                'file': (filename, open(filepath, 'rb'), content_type),
                'test_run_id': test_run_id,
                'file_key': filename,
            }
        )
        resp = self.get(BASE_URL + "/upload/geturl")
        headers = {'Content-Type': multipartblob.content_type}
        response = self.post(resp.text, data=multipartblob,
                             headers=headers)
        if response.status_code == 200:
            logger.info("File upload for %s succeeded!" % filename)
            os.remove(filepath)
        else:
            logger.info("File upload for %s failed" % filename)

    def update_command_status(self, status):
        c_url = BASE_URL + '/commands'
        data = json.dumps({
                'command_id': self.test_run_id,
                'status': status,
            })
        self.put(c_url, data)

    def transmit_slices(self):
        # post slices
        self.update_command_status('transmitting slices')
        slice_dir = os.path.join(gateway_helpers.DIRPATH, 'slices')
        if not os.path.exists(slice_dir):
            os.makedirs(slice_dir)
        voltage_start_time = 0
        dict_of_slice_lists = collections.defaultdict(int)
        list_of_slices = []
        for ch in self.ce_dict['enabled_list']:
            voltages = self.trace_dict[ch + '_voltages']
            # create list of slices
            list_of_slices = [voltages[x:x + int(CLIENT_SETTINGS['MAX_LENGTH_FOR_BROWSER'])]  # nopep8
                              for x in range(0, len(voltages), int(CLIENT_SETTINGS['MAX_LENGTH_FOR_BROWSER']))]  # nopep8
            dict_of_slice_lists[ch] = list_of_slices
            logger.debug("length of list of slices for %s: %s"
                         % (ch, len(list_of_slices)))
            self.more_metadata['total_points'] = len(voltages)
        for idx, slice_points in enumerate(list_of_slices):
            slices_by_channel = {}
            for ch in dict_of_slice_lists:
                if idx + 1 > len(dict_of_slice_lists[ch]):
                    logger.debug("""Warning! Slice index greater than length of
                        list of slices for %s""" % ch)
                else:
                    slices_by_channel[ch] = dict_of_slice_lists[ch][idx]
            voltage_start_time += self.time_step * len(slice_points)
            slice_data = {
                'test_run_id': self.trace_dict['test_run_id'],
                'command_id': self.trace_dict['command_id'],
                'slice_index': idx,
                'num_of_slices': len(list_of_slices),
                'voltage_start_time': voltage_start_time,
                'time_step': self.time_step,
                'test_results': slices_by_channel,
            }
            self.slices.append(slice_data)
            slice_data = zlib.compress(json.dumps(slice_data))
            filename = self.command_id + '-slice-' + str(idx) + '.json.gz'
            slice_file = os.path.join(slice_dir, filename)
            with open(slice_file, 'w') as f:
                f.write(slice_data)
        self.more_metadata['num_of_slices'] = len(list_of_slices)
        self.more_metadata['slice_length'] = int(CLIENT_SETTINGS['MAX_LENGTH_FOR_BROWSER'])  # nopep8
        self.transmit_slices_in_dir()

    def transmit_slices_in_dir(self, slice_dir=None):
        if not slice_dir:
            slice_dir = os.path.join(gateway_helpers.DIRPATH, 'slices')
        if not os.path.exists(slice_dir):
            return
        for filename in os.listdir(slice_dir):
            if '-slice-' in filename and filename.endswith(".gz"):
                slice_file = os.path.join(slice_dir, filename)
                self.transmit_file(slice_file)
                command_id = filename.split('-slice-')[0]
                slice_idx = filename.split('-slice-')[-1].split('.')[0]
                metadata = {
                    'command_id': command_id,
                    'slice_index': slice_idx,
                    'ready': True,
                }
                url = BASE_URL + '/results/' + command_id + '/slices/metadata'
                self.put(url, data=json.dumps(metadata))

    def transmit_config(self):
        """Posts config to server for storage"""
        if 'raw_setup' in self.trace_dict:
            info = {'raw_setup': self.trace_dict['raw_setup']}
        else:
            info = {}
        payload = {
            'config_excerpt': self.trace_dict['config_excerpt'],
            'config_data': {
                'new_config_name': self.trace_dict['test_run_id'],
                'info': info,
                'instrument_type': self.trace_dict['instrument_type'],
                'company_nickname': COMMON_SETTINGS['COMPANYNAME'],
            }
        }
        create_config_url = BASE_URL + "/create_config"
        _response = self.post(
            url=create_config_url,
            data=json.dumps(payload),
            headers=get_headers())
        assert _response.status_code == 200

    def transmit_logs(self, to_blob=False):
        filename = str(self.test_run_id) + '.log'
        if to_blob:
            # TODO: add functionality
            # transmit_logs(test_run_id=self.test_run_id)
            return
        # else it will just go to memcache
        multipartblob = MultipartEncoder(
            fields={
                'logfile': (filename, open('client.log', 'rb'), 'text/plain'),
                'testrunid': str(self.test_run_id),
            }
        )
        log_url = "https://" + \
            COMMON_SETTINGS['DOMAIN'] + "/logs/" + str(self.test_run_id)
        headers = {'Content-Type': multipartblob.content_type}
        resp = self.post(log_url, data=multipartblob, headers=headers)
        logger.debug(resp.text)

    def transmit_trace(self):
        """results transmission for scopes"""
        self.session = requests.session()
        self.session.headers.update(gateway_helpers.get_headers())
        try:
            logger.debug("transmitting trace")
            # to memcache
            self.transmit_slices()

            # complete transmission indexing blobstore data
            self.test_complete()
        except Exception:
            logger.error("Exception occurred during trace transmission")
            logger.error(traceback.format_exc())
            SENTRY_CLIENT.captureException()
        finally:
            tid = self.trace_dict['test_run_id']
            self.post_logfile(test_run_id=tid)

    def shrink(self, voltage_list, time_step, mode="normal", limit=400):
        len_voltage_list = len(voltage_list)
        dec_factor = len_voltage_list / int(limit)
        if dec_factor == 0:
            dec_factor = 1
        new_time_step = dec_factor * float(time_step)
        shrunk_list = []
        index = 0
        while index < len_voltage_list:
            if mode == "normal":
                shrunk_list.append(voltage_list[index])
                index += dec_factor
            else:  # implement other modes here
                pass
        shrunk_data = {
            'results': shrunk_list,
            'time_step': new_time_step,
        }
        return shrunk_data

    def generate_thumbnail_data(self):
        try:
            filename = 'full-trace-%s.json' % self.test_run_id
            infile = os.path.join(TMPDIR, filename)
            with open(infile, 'r') as f:
                trace_data = json.loads(f.read())
            channels = []
            for ch in trace_data['channels']:
                channels.append(self.shrink(ch['results'], ch['time_step']))
            return {'channels': channels}
        except Exception as e:
            logger.warning(e, exc_info=True)

    def post_result(self):
        thumbnail_data = self.generate_thumbnail_data()
        PAYLOAD = {
            # 'result' is for the datastore entity
            'result': {
                'id': self.results_dict['test_run_id'],
                'command_id': self.results_dict['test_run_id'],
                'config': self.results_dict['config_name'],
                'instrument_type': self.results_dict['instrument_type'],
                'tags': self.results_dict['tags'],
                'plan': self.results_dict['plan'],
                'info': self.results_dict,  # ToDo: remove unnecessary values
                'dut': self.results_dict['dut'],
            },
            'fields': [
                {
                    'name': 'config_name',
                    'value': self.results_dict['config_name'],
                    'type': 'text'
                },
                {
                        'name': 'hardware_name',
                        'value': self.results_dict['hardware_name'],
                        'type': 'text'
                },
                {
                        'name': 'instrument_type',
                        'value': self.results_dict['instrument_type'],
                        'type': 'text'
                },
                {
                        'name': 'test_run_id',
                        'value': self.results_dict['test_run_id'],
                        'type': 'text'
                },
                {
                        'name': 'start_datetime',
                        'value': str(datetime.now()),
                        'type': 'text'
                },
                {
                        'name': 'thumbnail_json',
                        'value': json.dumps(thumbnail_data),
                        'type': 'text'
                }
            ],
            'index_name': 'results'
        }

        url = BASE_URL + "/results"
        self.post(url, data=json.dumps(PAYLOAD))


class GRLTransmitter(ScopeTransmitter):
    """transmits data to server for GRL tests"""

    def __init__(self, transformer, trace_data):
        ScopeTransmitter.__init__(transformer, trace_data)

    def transmit_slices(self):
        # post slices
        headers = {'Content-Type': 'application/json', 'Accept': 'text/plain'}
        slice_url = BASE_URL + '/results/' + self.test_run_id + '/slices'
        voltage_start_time = 0
        dict_of_slice_lists = collections.defaultdict(int)
        ch = 'ch1'
        voltages = self.trace_dict[ch + '_voltages']
        # create list of slices
        list_of_slices = [voltages[x:x + int(CLIENT_SETTINGS['MAX_LENGTH_FOR_BROWSER'])]  # nopep8
                          for x in range(0, len(voltages), int(CLIENT_SETTINGS['MAX_LENGTH_FOR_BROWSER']))]  # nopep8
        dict_of_slice_lists[ch] = list_of_slices
        logger.debug("length of list of slices for %s: %s" %
                     (ch, len(list_of_slices)))
        for slice_index, slice_points in enumerate(list_of_slices):
            slices_by_channel = {}
            for ch in dict_of_slice_lists:
                if slice_index + 1 > len(dict_of_slice_lists[ch]):
                    logger.debug("""Warning! Slice index greater than length of
                        list of slices for %s""" % ch)
                else:
                    slices_by_channel[ch] = dict_of_slice_lists[
                        ch][slice_index]
            voltage_start_time += self.time_step * len(slice_points)
            slice_data = {
                'test_run_id': self.trace_dict['test_run_id'],
                'slice_index': slice_index,
                'num_of_slices': len(list_of_slices),
                'voltage_start_time': voltage_start_time,
                'time_step': self.time_step,
                'test_results': slices_by_channel,
            }
            slice_data = json.dumps(slice_data, ensure_ascii=True)
            if cfg.getboolean('client', 'USE_GZIP'):
                headers['content-encoding'] = 'gzip'
                slice_data = zlib.compress(json.dumps(slice_data))
            self.post(slice_url, data=slice_data, headers=headers)

    def transmit_result_blob(self, infile='grl-data.json'):
        super(GRLTransmitter, self).transmit_result_blob(infile)
