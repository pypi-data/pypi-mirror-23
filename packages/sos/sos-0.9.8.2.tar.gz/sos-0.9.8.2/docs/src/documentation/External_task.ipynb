{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Executing external tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "output_cache": "[]"
   },
   "source": [
    "## SoS Task Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SoS provides mechanisms to execute all processing steps, regardless of languages of scripts, requirements of  resources (CPU, memory usage etc), and execution environments in a single workflow or Jupyter notebook. The basis of this flexibility (and resulging reproducibility) is a task model that allows you to execute part of the workflow on remote hosts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the SoS task model, you can easily define part of step processes as tasks and execute them externally. The tasks are\n",
    "\n",
    "* **self-contained** in that they contain all the required information to be executed anywhere.\n",
    "* **generated and executed dynamically** in that a task would only be generated when all its dependencies have been met.\n",
    "* **independent of workflows and other tasks**. Tasks are defined by the jobs they are performing and can be shared by different workflows if they happen to perform exactly the same function.\n",
    "* **can be executed on remote hosts or task queues**. Tasks can be executed directly on a local or remote host, to task queues such as Celery, or be submitted to batch systems such as PBS/Torch, Slurm, and IBM LSF.\n",
    "* **(mostly) independent of file systems**. SoS automatically synchronize input, output files and specified files between local and remote systems so you can easily switch from one remote host to another.\n",
    "* **support remote targets**. If input, depends, or output files are large, you also have the option to process remote targets directly without synchronizing them to local host."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "output_cache": "[]"
   },
   "source": [
    "The following figure illustrates the task model of SoS\n",
    "\n",
    "![job queue](../media/job_queue.svg )\n",
    "\n",
    "Basically,\n",
    "1. Tasks are part of step processes.\n",
    "2. Tasks are managed by task engines, multiple task engines can be used for a single workflow.\n",
    "3. Task engines generate task files, submit tasks, monitor task status, and return results to SoS workflow.\n",
    "4. Remote task engines synchronize input files, translate and copy tasks to server, and start the tasks on the remote server."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "output_cache": "[]"
   },
   "source": [
    "## Specification of tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A **step process** consists of everything after the `input` statement. It can be repeated with different **input groups** defined by input options `group_by` or `for_each`. For example, if `bam_files` is a list of bam files,\n",
    "\n",
    "```\n",
    "[10]\n",
    "input: bam_files, group_by=1\n",
    "output: \"${_input}.bai\"\n",
    "\n",
    "run:\n",
    "    samtools index ${_input}\n",
    "```\n",
    "\n",
    "execute a shell script to process each bam file. This is done sequentially for each input file, and is performed by SoS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can easily specify part or all of a step process as **tasks**, by prepending the statements with a `task` keyword:\n",
    "\n",
    "\n",
    "```\n",
    "[10]\n",
    "input: bam_files, group_by=1\n",
    "output: \"${_input}.bai\"\n",
    "\n",
    "task:\n",
    "run:\n",
    "    samtools index ${_input}\n",
    "```\n",
    "\n",
    "This statement declares the rest of the step process as a `task`. For each input file, a task will be created with an ID determined from task content and context (input and output files, variables etc). The task will be by default executed by a local `process` task queue where tasks are started as background processes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The benefit of executing tasks externally is that the tasks are executed concurrently, on the local machine or a remote server, or be submitted to a task queue. For example, in the previous example, multiple tasks could be executed in parallel (but on the same machine) unless you specify it otherwise as follows\n",
    "\n",
    "```\n",
    "[10]\n",
    "input: bam_files, group_by=1\n",
    "output: \"${_input}.bai\"\n",
    "\n",
    "task: concurrent=False\n",
    "run:\n",
    "    samtools index ${_input}\n",
    "```\n",
    "\n",
    "You can also use command\n",
    "\n",
    "```\n",
    "sos run myscript -q cluster\n",
    "```\n",
    "or use option `queue`\n",
    "```\n",
    "[10]\n",
    "input: bam_files, group_by=1\n",
    "output: \"${_input}.bai\"\n",
    "\n",
    "task: queue='cluster'\n",
    "run:\n",
    "    samtools index ${_input}\n",
    "```\n",
    "\n",
    "to submit the commands to a cluster system to be executed on different computing nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "output_cache": "[]"
   },
   "source": [
    "## Configure `hosts`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "output_cache": "[]"
   },
   "source": [
    "A server or task queue needs to be defined in SoS configuration file. SoS automatically loads global configuration files `~/.sos/hosts.yml`, `~/.sos/config.yml`, and a local configuration file `./config.yml`. You can also put your settings in any configuration file and specify it with option `-c`. `~/.sos/hosts.yml` is designed to record all host-related configurations and should be used for this purpose.\n",
    "\n",
    "The configuration file could be edited manually if you are familiar with the YAML format. Otherwise you can use the `sos config` command to add or modify settings. For example, command\n",
    "\n",
    "```bash\n",
    "sos config --hosts --set hosts.shark.address username@shark.com\n",
    "```\n",
    "\n",
    "would write to `~/.sos/hosts.yml` the following content\n",
    "\n",
    "```\n",
    "hosts:\n",
    "    shark:\n",
    "        address: username@shark.com\n",
    "```\n",
    "\n",
    "This effectively defines a host with alias `shark`. All configurations related to this host should be defined under `shark`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "output_cache": "[]"
   },
   "source": [
    "### Define `localhost`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "output_cache": "[]"
   },
   "source": [
    "You will need to configure both localhost and server to be able to send local jobs to remote servers or task queues. A `localhost` should be defined to point to an existing host definition. For example, if a `OfficeDesktop` is defined in `~/.sos/hosts.yml` as \n",
    "\n",
    "```\n",
    "hosts:\n",
    "    OfficeDesktop:\n",
    "        paths:\n",
    "            home: /Users/myuser\n",
    "    cluster:\n",
    "        address: url@cluster\n",
    "        paths:\n",
    "            home: /home/myuser\n",
    "             \n",
    "```\n",
    "\n",
    "You should use command\n",
    "\n",
    "```\n",
    "sos config --global --set localhost OfficeDesktop\n",
    "```\n",
    "\n",
    "to define `localhost` as `OfficeDesktop` on your office desktop, and define `localhost` as `cluster` on the cluster if you would like to submit jobs also from the cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "output_cache": "[]"
   },
   "source": [
    "### `address`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "output_cache": "[]"
   },
   "source": [
    "IP address or URL of the host. Note that\n",
    "\n",
    "* `address` should be ignored for hosts, for example your desktop, that do not accept remote execution.\n",
    "* If you have a different user name on the the remote host, specify the `address` in the format of `username@hostaddress`.\n",
    "* SoS does not support username/password authentication so **public key authentication between local and remote hosts is reuired for communication between local and remote host**.\n",
    "* SoS currently does not support remote execution on windows hosts so no `address` is needed for windows hosts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "output_cache": "[]"
   },
   "source": [
    "### `port`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "output_cache": "[]"
   },
   "source": [
    "ssh port of the remote host, which is `22` by default. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "output_cache": "[]"
   },
   "source": [
    "### `shared`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "output_cache": "[]"
   },
   "source": [
    "Option `shared` tells SoS which file systems are shared between localhost and some remote hosts so that it does not have to synchronize files under these directories between the hosts.\n",
    "\n",
    "The `shared` entry should be defined with `name` and `path` pairs in the format of\n",
    "\n",
    "```\n",
    "hosts:\n",
    "    desktop:\n",
    "    server:\n",
    "        shared:\n",
    "            project: /myprojects\n",
    "            HTFS: /\n",
    "    worker:\n",
    "        shared:\n",
    "            HTFS: /\n",
    "    server1:\n",
    "        shared:\n",
    "            project: /scratch/myprojects\n",
    "            data: /scratch/data\n",
    "````\n",
    "\n",
    "The above cooked configuration says:\n",
    "\n",
    "1. `desktop` does not share any volume with any other machine so all files need to be transferred.\n",
    "2. `server` and `worker` shares `HTFS` with directory `/`, so all files are shared.\n",
    "3. `server` and `server1` share a `project` volume but the volume is mounted at different locations. So files under `myprojects` are not synchronized if you are submitting jobs from `server` to `server1`, and files under `/scratch/myprojects` are not synchronized if you are submitting jobs from `server1` to `server`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "output_cache": "[]"
   },
   "source": [
    "### `paths`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "output_cache": "[]"
   },
   "source": [
    "`paths` defines paths that will be translated when a task is executed remotely. For example, your input file on a mac might be `/Users/myuser/project/KS28.fa`, but it should be named `/home/myuser/project/KS28.fa` if it is processed on a remote server. In this case, you should define directories `/Users/myuser` and `/home/myuser` as equivalent directories on the two hosts, using \n",
    "\n",
    "```\n",
    "hosts:\n",
    "    desktop:\n",
    "        paths:\n",
    "            home: /Users/myuser\n",
    "    server:\n",
    "        paths:\n",
    "            home: /home/myuser\n",
    "```\n",
    "\n",
    "Multiple entried could be defined and the files would be mapped by the longest mapping path. For example, if you have on the server a shared location for all resources, you could define\n",
    "\n",
    "```\n",
    "hosts:\n",
    "    desktop:\n",
    "        paths:            \n",
    "            home: /Users/myuser\n",
    "            resource: /Users/myuser/resources\n",
    "    server:\n",
    "        paths:\n",
    "            home: /home/myuser\n",
    "            resource: /shared/resources\n",
    "```\n",
    "\n",
    "so that `/Users/myuser/resources/hg19.fa` could be mapped to `/shared/resources/hg19.fa` on the server. Note that `/Users/myuser/resource/hg19.fa` would be matched to `resource` instead of `home` because `resource` matches longer piece of the input path.\n",
    "\n",
    "A remote host can be accessible from a local host only if the remote host defines all paths defined by the local host. More specifically, if host A defines path `home` and host B defines paths `home` and `resource`, it is possible to connect from host A to host B using `home`, but not from host B to A because SoS does not know how to map paths under `resource`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "output_cache": "[]"
   },
   "source": [
    "### `path_map` (derived)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "output_cache": "[]"
   },
   "source": [
    "With definitions of `paths` on both hosts, SoS would derive a set of `path_maps` between all hosts. Actually, when you run \n",
    "\n",
    "```\n",
    "sos status -q\n",
    "```\n",
    "\n",
    "to list all host configurations, SoS would list all hosts accessible from `localhost`, with host-specific `path_map`, which is a list of directory mappings between local and remote directories. For example, the `path_map` from `desktop` to `server` using the above example would be\n",
    "\n",
    "```\n",
    "/Users/myuser -> /home/myuser\n",
    "/Users/myuser/resoruces -> /shared/resources\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "output_cache": "[]"
   },
   "source": [
    "## Common queue configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Options such as `address` and `paths` specify on to access, synchronize files and execute commands on a remote host. The following options specify how tasks should be executed on it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "output_cache": "[]"
   },
   "source": [
    "### `queue_type`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "output_cache": "[]"
   },
   "source": [
    "Option `query_type` determines the type of remote server or job queue. SoS currently supports the following types of job queues:\n",
    "\n",
    "1. **`process`**: this is the default queue type. Tasks are executed directly, either on local host or on a server.\n",
    "2. **`pbs`**: A PBS/MOAB/LFS/Slurm cluster system where tasks are submitted using commands such as `qsub`.\n",
    "3. **`rq`**: A redis queue where tasks are submitted to the rq server and monitored through rq-dashboard.\n",
    "4. **`celery`**: A celery queue where tasks are submitted to the celery server and monitored through celery's flower module."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "output_cache": "[]"
   },
   "source": [
    "### `wait_for_task`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "output_cache": "[]"
   },
   "source": [
    "Whether or not SoS should wait for the completion of tasks (`true/false`). By default SoS will wait for the completion of tasks submitted in a queue and complete the workflow in one run. Alternatively, SoS would quit after all tasks have been submitted to a queue with `wait_for_task` set to `false`. The workflow can be resumed after tasks have been completed.\n",
    "\n",
    "This option is set to `True` by default and can be overridden by command line option `-w` (wait) or `-W` (no wait)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "output_cache": "[]"
   },
   "source": [
    "### `status_check_interval`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "output_cache": "[]"
   },
   "source": [
    "Option `status_check_interval` determines frequency at which SoS checks status of jobs. This is set by default to 2 seconds for `process` queue type, and `10` seconds for all other types. This number should be set to at least `60` for remote servers and longer jobs because it can be a burden to query the status of jobs very frequently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "output_cache": "[]"
   },
   "source": [
    "### `max_running_jobs`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "output_cache": "[]"
   },
   "source": [
    "Maximum number of running jobs. This setting controls how SoS releases tasks to job queues and is independent of possible maximum running job settings of individual task queues.\n",
    "\n",
    "This option is set to 10 by default and can be overriden by command line option `-J`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `max_cores`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maximum number of processes (an integer) allowed for the queue. A task would fail immediately if it specifies more `cores` than this option. For queues that do not require a `cores` option (e.g. a `process` queue), tasks that use more processes than `max_cores` will be automatically killed.\n",
    "\n",
    "This option, along with `max_walltime` and `max_mem` that are described below, help prevent the submission of erraneous tasks to the queue. It also helps prevent the execution large jobs on wrong servers, such as the headnode of a cluster system on which only job preparation and submission steps are allowed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `max_walltime`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Maximum walltime allowed for the queue. `max_walltime` could be specified as a string in the format of `HH:MM:SS`, or an integer with units `s` (second), `m` (minute), `h` (hour), or `d` (day). A task would fail immediately if it specifies more `walltime` than this option. For queues that do not require a `walltime` option (e.g. a `process` queue), tasks that has exceeded `max_walltime` will be automatically killed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `max_mem`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maximum memory comsumption allowed for the queue. `max_mem` could be specified as an integer (bytes) or a string with units such as `M`, `G`, `MB` etc. A task would fail immediately if it specifies more `mem` than this option. For queues that do not require a `mem` option (e.g. a `process` queue), tasks that use more RAM than `max_mem` will be automatically killed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "output_cache": "[]"
   },
   "source": [
    "### `description`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "output_cache": "[]"
   },
   "source": [
    "An optional short description for the queue that will be displayed with commands such as `sos status -q`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "output_cache": "[]"
   },
   "source": [
    "### Arbitrary key value pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "output_cache": "[]"
   },
   "source": [
    "You can define arbitrary key value pairs in the host configuration. These variables could be used for the interpolation of commands and templates. For example, if you define\n",
    "\n",
    "```\n",
    "queue: long\n",
    "```\n",
    "\n",
    "You could use \n",
    "\n",
    "```bash\n",
    "#PBS -q ${queue}\n",
    "```\n",
    "\n",
    "in your PBS job templates (configuration `job_template`).\n",
    "\n",
    "If the name of a variable matches a runtime variable (e.g. `mem`, `walltime`), its value would be overriden by corresponding runtime variable. This feature can be used to define default values for runtime variables. For example,\n",
    "\n",
    "```\n",
    "walltime = '10:00:00'\n",
    "job_template = |\n",
    "    #PBS -l walltime=${walltime}\n",
    "```\n",
    "\n",
    "would set a default `walltime` of 10 hours for tasks without specified `walltime`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample remote server configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the default `queue_type: process`, the following configuration allows you to execute tasks on a remote server. You can change `wait_on_task` to `true` if you would like to wait for the completion of tasks and be able to kill runing tasks easily.\n",
    "\n",
    "```\n",
    "    computing_server:\n",
    "        address: username@host.url\n",
    "        max_mem: 20G\n",
    "        max_procs: 12\n",
    "        max_walltime: 10h\n",
    "        wait_on_task: false\n",
    "        paths:\n",
    "            home: /home/username\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "output_cache": "[]"
   },
   "source": [
    "## RQ configuration (pending)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "output_cache": "[]"
   },
   "source": [
    "### `redis_host`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "output_cache": "[]"
   },
   "source": [
    "Address of the redis server, default to `localhost`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "output_cache": "[]"
   },
   "source": [
    "### `redis_port`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "output_cache": "[]"
   },
   "source": [
    "Port of the redis server, default to `6379`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "output_cache": "[]"
   },
   "source": [
    "## Celery configuration (pending)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "output_cache": "[]"
   },
   "source": [
    "### `broker`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "output_cache": "[]"
   },
   "source": [
    "`broker` configuration of celery app."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "output_cache": "[]"
   },
   "source": [
    "### `backend`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "output_cache": "[]"
   },
   "source": [
    "`backend` configuration of the celery app."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "output_cache": "[]"
   },
   "source": [
    "## PBS/Torch configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "output_cache": "[]"
   },
   "source": [
    "### `job_template`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "output_cache": "[]"
   },
   "source": [
    "Option `job_template` is a template of a shell script that will be submitted to the PBS system. A typical template would be specified as (using a multi-line string literal of YAML format)\n",
    "\n",
    "```bash\n",
    "hosts:\n",
    "  server:\n",
    "    job_template: |\n",
    "      #!/bin/bash\n",
    "      #PBS -N ${job_name}\n",
    "      #PBS -l nodes=1:ppn=${cores}\n",
    "      #PBS -l walltime=${walltime}\n",
    "      #PBS -l mem=${mem//10**6}MB\n",
    "      #PBS -o ${cur_dir}/${task}.out\n",
    "      #PBS -e ${cur_dir}/${task}.err\n",
    "      #PBS -q long\n",
    "      #PBS -m ae\n",
    "      #PBS -M your@email.address\n",
    "      #PBS -v ${cur_dir}\n",
    "      \n",
    "      cd ${cur_dir}\n",
    "      \n",
    "      sos execute ${task} -v ${verbosity} -s ${sig_mode} \\\n",
    "        ${'--dryrun' if run_mode == 'dryrun' else ''}\n",
    "```\n",
    "\n",
    "The template will be interpolated with the following information\n",
    "\n",
    "* `task`: task id\n",
    "* `job_name`: Interpolated value of runtime option `name`, or task id if `name` is not provided. For example, `name='${step_name}_${_index}'` would assign names with step name and index to submitted jobs.\n",
    "* `nodes`, `cores`, `walltime`, `mem`: resource task options\n",
    "* `cur_dir`:  current project directory, which will be translated to path in remote host if the task is executed remotely\n",
    "* `verbosity` and `sig_mode`: sos run mode.\n",
    "* `run_mode` to allow the script to be executed in dryrun mode, in which mode scripts would be printed instead of executed. It is very important to set this option because the job script would be executed directly (on head node) instead of sent to the PBS queue if sos is running in dryrun mode (`sos run -q pbs -n`).\n",
    "* Other key/value pairs you defined for the server\n",
    "\n",
    "Note that\n",
    "1. You will need to specify resource options (`nodes`, `cores`, `walltime`, and `mem`) as task options if they are used in the job template without default values.\n",
    "2. If you need to specify more options (e.g. queue name), you can define multiple host entries with different options, for example, a `cluster-short` and a `cluster-long` on the same cluster.\n",
    "3. Alternatively, it is possible to derive the options from existing runtime options. For example, you could put a task to long queue if it runs more than 24 hours\n",
    "\n",
    "    ```\n",
    "    #PBS -q ${'long' if int(walltime.split(':')[0]) > 24 else 'short'}\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "output_cache": "[]"
   },
   "source": [
    "### `submit_cmd`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "output_cache": "[]"
   },
   "source": [
    "A `submit_cmd` template is the command that will be executed to submit the job. It accepts the same set of variables as `job_template`, with an additional variable `job_file` pointing to the location of the job file on the remote host. The `submit_cmd` is usually as simple as\n",
    "\n",
    "```bash\n",
    "qsub ${job_file}\n",
    "```\n",
    "\n",
    "but you could specify some options from command line instead of the job file and define `submit_cmd` as\n",
    "\n",
    "```bash\n",
    "msub -l ${walltime} < ${job_file}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "output_cache": "[]"
   },
   "source": [
    "### `submit_cmd_output`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "output_cache": "[]"
   },
   "source": [
    "This option specifies the output of the `submit_cmd` command and let SoS know how to extract `job_id` and other information from it. For example, for a regular PBS system, the output is simply the `job_id` (stripping spaces and newlines).\n",
    "\n",
    "```\n",
    "submit_cmd_output='{job_id}'\n",
    "```\n",
    "\n",
    "On a LSF system, the output should be similar to\n",
    "\n",
    "```\n",
    "submit_cmd_output='Job <{job_id}> is submitted to queue <{queue}>'\n",
    "```\n",
    "\n",
    "The information extracted (namely variables defined) from `submit_cmd` can be used for other commands such as `status_cmd`. Note that this parameter uses pattern matching `{ }` to extract variable, instead of `${ }` to use variables.\n",
    "\n",
    "This option is defult to `{job_id}`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "output_cache": "[]"
   },
   "source": [
    "### `status_cmd`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "output_cache": "[]"
   },
   "source": [
    "An command to query the status of a submitted task. For a standard PBS system, this option could be\n",
    "\n",
    "```\n",
    "qstat ${job_id}\n",
    "```\n",
    "\n",
    "where `job_id` is the output of command `submit_cmd`. The `status_cmd` is interpolated with variables `job_id` (PBS job ID), `task` (SoS task id), and `verbosity` (command line verbosity level) so you would adjust options for different verbosity level (e.g. `${'-f' if verbosity > 2 else ''}`).\n",
    "\n",
    "Note that the `status_cmd` is only called with `-v 2` or higher."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "output_cache": "[]"
   },
   "source": [
    "### `kill_cmd`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "output_cache": "[]"
   },
   "source": [
    "A command to kill a submitted job on the cluster. For a standard PBS system, this option could be\n",
    "\n",
    "```\n",
    "qdel ${job_id}\n",
    "```\n",
    "where `job_id` is the output of command `submit_cmd`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample PBS/Torch configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "    cluster:\n",
    "        address: host.url\n",
    "        description: cluster with PBS\n",
    "        paths:\n",
    "            home: /scratch/username\n",
    "        queue_type: pbs\n",
    "        status_check_interval: 30\n",
    "        wait_for_task: false\n",
    "        job_template: |\n",
    "            #!/bin/bash\n",
    "            #PBS -N ${task}\n",
    "            #PBS -l nodes=${nodes}:ppn=${ppn}\n",
    "            #PBS -l walltime=${walltime}\n",
    "            #PBS -l mem=${mem//10**9}GB\n",
    "            #PBS -o ${home_dir}/.sos/tasks/${task}.out\n",
    "            #PBS -e ${home_dir}/.sos/tasks/${task}.err\n",
    "            #PBS -m ae\n",
    "            #PBS -M bpeng@mdanderson.org\n",
    "            #PBS -v ${cur_dir}\n",
    "            cd ${cur_dir}\n",
    "            sos execute ${task} -v ${verbosity} -s ${sig_mode} ${'--dryrun' if run_mode == 'dryrun' else ''}        max_running_jobs: 100\n",
    "        submit_cmd: qsub ${job_file}\n",
    "        status_cmd: qstat ${job_id}\n",
    "        kill_cmd: qdel ${job_id}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample MOAB configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "    cluster:\n",
    "        address: host.url\n",
    "        description: cluster with MOAB\n",
    "        paths:\n",
    "            home: /scratch/username\n",
    "        queue_type: pbs\n",
    "        status_check_interval: 30\n",
    "        wait_for_task: false\n",
    "        job_template: |\n",
    "            #!/bin/bash\n",
    "            #PBS -N ${task}\n",
    "            #PBS -l nodes=${nodes}:ppn=${ppn}\n",
    "            #PBS -l walltime=${walltime}\n",
    "            #PBS -l mem=${mem//10**9}GB\n",
    "            #PBS -o ${home_dir}/.sos/tasks/${task}.out\n",
    "            #PBS -e ${home_dir}/.sos/tasks/${task}.err\n",
    "            #PBS -m ae\n",
    "            #PBS -M bpeng@mdanderson.org\n",
    "            #PBS -v ${cur_dir}\n",
    "            cd ${cur_dir}\n",
    "            sos execute ${task} -v ${verbosity} -s ${sig_mode} ${'--dryrun' if run_mode == 'dryrun' else ''}        max_running_jobs: 100\n",
    "        submit_cmd: msub ${job_file}\n",
    "        status_cmd: qstat ${job_id}\n",
    "        kill_cmd: qdel ${job_id}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample LFS configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "    lfs:\n",
    "        address: host.url\n",
    "        description: cluster with LSF\n",
    "        paths:\n",
    "            home: /rsrch2/bcb/bpeng1/\n",
    "            project: /scratch/bcb/bpeng1/RNASeq\n",
    "        queue_type: pbs\n",
    "        status_check_interval: 30\n",
    "        wait_for_task: false\n",
    "        job_template: |\n",
    "            #!/bin/bash\n",
    "            #BSUB -J ${job_name}\n",
    "            #BSUB -q ${'short' if int(walltime.split(':')[0]) < 24 else 'long'}\n",
    "            #BSUB -n ${cores}\n",
    "            #BSUB -M ${mem//10**9}G\n",
    "            #BSUB -W 1:0\n",
    "            #BSUB -o ${home_dir}/.sos/tasks/${task}.out\n",
    "            #BSUB -e ${home_dir}/.sos/tasks/${task}.err\n",
    "            #BSUB -N\n",
    "            #BSUB -u bpeng@mdanderson.org\n",
    "            cd ${cur_dir}\n",
    "            sos execute ${task} -v ${verbosity} -s ${sig_mode} ${'--dryrun' if run_mode == 'dryrun' else ''}\n",
    "        max_running_jobs: 100\n",
    "        submit_cmd: bsub < ${job_file}\n",
    "        submit_cmd_output: 'Job <{job_id}> is submitted to queue <{queue}>'\n",
    "        status_cmd: bjobs ${job_id}\n",
    "        kill_cmd: bkill ${job_id}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample SLURM configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This configuration defines default `walltime` and `cores`.\n",
    "\n",
    "```\n",
    "mstephens:\n",
    "    description: cluster with SLURM\n",
    "    address: host.url\n",
    "    paths:\n",
    "      home: /home/username\n",
    "    queue_type: pbs\n",
    "    status_check_interval: 120\n",
    "    max_running_jobs: 15\n",
    "    max_cores: 28 \n",
    "    max_walltime: \"36:00:00\"\n",
    "    max_mem: 256G\n",
    "    job_template: |\n",
    "      #!/bin/bash\n",
    "      #SBATCH --time=${walltime}\n",
    "      #SBATCH --partition=mstephens\n",
    "      #SBATCH --account=pi-mstephens\n",
    "      #SBATCH --nodes=1\n",
    "      #SBATCH --ntasks-per-node=${cores}\n",
    "      #SBATCH --mem-per-cpu=${mem_per_cpu}\n",
    "      #SBATCH --job-name=${job_name}\n",
    "      #SBATCH --output=${cur_dir}/.sos/${job_name}.out\n",
    "      #SBATCH --error=${cur_dir}/.sos/${job_name}.err\n",
    "      cd ${cur_dir}\n",
    "      mkdir -p .sos\n",
    "      sos execute ${task} -v ${verbosity} -s ${sig_mode} ${'--dryrun' if run_mode == 'dryrun' else ''}\n",
    "    walltime: \"06:00:00\"\n",
    "    cores: 20\n",
    "    mem_per_cpu: 1000\n",
    "    submit_cmd: sbatch ${job_file}\n",
    "    submit_cmd_output: \"Submitted batch job {job_id}\"\n",
    "    status_cmd: squeue --job ${job_id}\n",
    "    kill_cmd: scancel ${job_id}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample Task Spooler configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Task Spooler](http://vicerveza.homeunix.net/~viric/soft/ts/) is a light-weight task spooler for single machines.\n",
    "\n",
    "```\n",
    "    taskspooler:\n",
    "        description: task spooler on a single machine\n",
    "        address: username@host.url\n",
    "        port: 32771\n",
    "        paths:\n",
    "            home: /home/user\n",
    "        queue_type: pbs\n",
    "        status_check_interval: 5\n",
    "        job_template: |\n",
    "            #!/bin/bash\n",
    "            cd ${cur_dir}\n",
    "            sos execute ${task} -v ${verbosity} -s ${sig_mode} ${'--dryrun' if run_mode == 'dryrun' else ''}\n",
    "        max_running_jobs: 100\n",
    "        submit_cmd: tsp -L ${task} sh ${job_file}\n",
    "        status_cmd: tsp -s ${job_id}\n",
    "        kill_cmd: tsp -r ${job_id}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "output_cache": "[]"
   },
   "source": [
    "## Task options"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following options are options to keyword `task:` and specify how tasks should be executed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "output_cache": "[]"
   },
   "source": [
    "* The resource options such as `walltime` and `cores` will be sent to individual task queues in appropriate format. You do not have to specify all options because task queues can support a subset of these options and some task queues provide default values (and some do not). It is however generally a good idea to specify them all so that your tasks could be executed on all types of task queues. \n",
    "\n",
    "* The execution options such as `workdir`, `env`, `concurrent` specify environments in which tasks will be submitted and executed. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "output_cache": "[]"
   },
   "source": [
    "### Option `walltime`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "output_cache": "[]"
   },
   "source": [
    "Estimated maximum running time of the task. This parameter will be sent to different task queues and it is up to the task queue to decide if the task would be killed if the task could not be completed within specified `walltime`. `walltime` could be specified as a string in the format of `HH:MM:SS` where `HH`, `MM` and `SS` are hours, minutes, and seconds, or an integer with units `s` (second), `m` (minute), `h` (hour), or `d` (day), although the internal format of `walltime` (when you use `walltime` in `job_template` etc) is always `HH:MM:SS`. For example, you could use `walltime='240:00:00'` or `walltime='10d'` for a job that would run 10 days."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "output_cache": "[]"
   },
   "source": [
    "### Option `cores`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "output_cache": "[]"
   },
   "source": [
    "Number of cores on each computing node, which corrsponds to the `ppn` option of a PBS system.\n",
    "\n",
    "PBS task queue also accepts a parameer `nodes` (corresponds to PBS resource option `nodes`, default to 1) but it is currently unused because SoS does not yet support multi-node tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "output_cache": "[]"
   },
   "source": [
    "### Option `mem`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "output_cache": "[]"
   },
   "source": [
    "The total amount of memory needed across all nodes. Default units are bytes; can also be expressed in megabytes (`mem=4000MB`). gigabytes (`mem=4GB`) or gibibytes (`mem=4GiB`), although all inputs are converted to bytes internally. To use this option in a `job_template`, you generally need to use expressions such as `${mem//1e9}GB`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "output_cache": "[]"
   },
   "source": [
    "### Option `name`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "output_cache": "[]"
   },
   "source": [
    "Name of the task, which can be a string such as `'task'` (for task ID) that would be interpolated when a task is generated. The value of this parameter has to be single quoted because otherwise the string would be interpolated before it is passed to task engine.\n",
    "\n",
    "This option has default `'${step_name}_${_index}'` and is currently only used to produce variable `job_name` for job template."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "output_cache": "[]"
   },
   "source": [
    "### Option `queue`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "output_cache": "[]"
   },
   "source": [
    "Option `queue` specifies a task queue to which the current task will be submitted. This option overrides system default (command line option `-q`) so it is generally a good idea to use command line option `-q` so that the task could be submitted to different task queues, unless the task has to be executed in a particular server (e.g. with a software that is unavailable elsewhere)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option `to_host`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Option `to_host` specifies additional files or directories that would be synchronized to the remote host before tasks are executed. It can be specified as\n",
    "\n",
    "* A single file or directory (with respect to local file system), or\n",
    "* A list of files or directories, or\n",
    "* A dictinary of `{local: remote}` file maps that specify how local files are synchronized to the remote host.\n",
    "\n",
    "In the first two cases, the files or directories will be translated using the host-specific path maps. In the last case, the `remote` path (that should be relative to the remote file system) will be used without translating `local` file.\n",
    "\n",
    "Note that \n",
    "1. If a symbolic link is specified in `to_host`, both the symbolic link and the path it refers to would be synchronized to the remote host.\n",
    "2. If the task is executed on the local host (remote host coincide with local host), `to_host` is usually ignored unless it is specified in the third dictionary format, which copies files to another location before task execution. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option `from_host`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Option `from_host` specifies additional files or directories that would be synchronized from the remote host after tasks are executed. It can be specified as\n",
    "\n",
    "* A single file or directory (with respect to local file system), or\n",
    "* A list of files or directories, or\n",
    "* A dictinary of `{local: remote}` file maps that specify how local files are synchronized from the remote host.\n",
    "\n",
    "In the first two cases, the files or directories will be translated using the host-specific path maps to determine what remote files to retrieve. In the last case, the `remote` path (that should be relative to the remote file system) will be used without path translation. If the task is executed on the local host (remote host coincide with local host), this option is usually ignored unless it is specified in the third dictionary format, which copies files to another location after the task is executed. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "output_cache": "[]"
   },
   "source": [
    "### Option `preserved_vars`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "output_cache": "[]"
   },
   "source": [
    "All variables in the task context environment, including implicit SoS variables such as `_input` and `_output` are automatically translated if they are string or sequence of strings (list, tuple etc). This would however translate variables such as `sample_name` with value `my_sample` to something like `/home/myuser/project/my_sample`. It is therefore important for you to identify all variables that should be preserved during context-switching in the option `preserved_vars`.\n",
    "\n",
    "This variables takes a list of variable names, but a string can be specified if there is only one name. That is to say, both\n",
    "\n",
    "```\n",
    "preserved_vars='sample_name'\n",
    "```\n",
    "\n",
    "or\n",
    "\n",
    "```\n",
    "preserved_vars=['sample_name', 'title']\n",
    "```\n",
    "\n",
    "are acceptable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option `trunk_size`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Options `trunk_size` and `trunk_workers` are useful for dividing a large number of small tasks into several larger tasks so that they can be executed efficiently on a cluster system.\n",
    "\n",
    "Option `trunk_size` groups concurrent tasks into trunks of specified size. For example, if you need to run 10000 simulations that each lasts about 1 minute, you can group the tasks into `10`  umbrella tasks, each running `1000` simulations.\n",
    "\n",
    "```\n",
    "[10]\n",
    "import random\n",
    "input: for_each={'seed': [random.randint(1, 10000000) for x in range(10000)]}\n",
    "task: mem=`1G`, walltime='1m', cores=2, trunk_size=1000\n",
    "sh:\n",
    "    run_simulation --seed $(seed) >> res_${seed}.res\n",
    "```\n",
    "\n",
    "The unbrella tasks have the following properties:\n",
    "1. Tasks embedded by an umbrella task are executed normally in the sense that they have their own input, output, task ID, signatures etc, although only the umbrella tasks are visible to task engines.\n",
    "2. Umbrella task IDs are prefixed by `M#_` where `#` is the number of embedded tasks.\n",
    "3. Umbrella tasks adjust resource options such as `walltime` automatically so in the above example, each umbrella task will have `walltime='16:40:00'` (1000 minutes). \n",
    "4. Option `name` (job name on PBS systems) will be adjusted to `${name)_##` (e.g. `default_10_6000_1000` if default `name='${step_name}_${_index}'` is used) where `##` is the number of subtasks.\n",
    "5. The entire umbrella will fail if any of the subtasks fails. However, since each subtask has its own signature, completed tasks will be ignored when you rerun the umbrella task (unless `-s force` is specified to forcefully re-execute all tasks)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "sos"
   },
   "source": [
    "### Option `trunk_workers`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Option `trunk_workers` specifies number of workers for umbrella tasks. If this option is specified, an umbrella task will be executed by a master process that dispatches embedded tasks to `trunk_workers` workers. Using the same simulation example with `trunk_workers=5`,\n",
    "\n",
    "```\n",
    "import random\n",
    "input: for_each={'seed': [random.randint(1, 10000000) for x in range(10000)]}\n",
    "task: mem=`1G`, walltime='1m', cores=2, trunk_size=1000, trunk_workers=5\n",
    "sh:\n",
    "    run_simulation --seed $(seed) >> res_${seed}.res\n",
    "```\n",
    "\n",
    "* There would be `10000 / 1000 = 10` umbrealla tasks each with `1000` (`trunk_size`) subtasks.\n",
    "* Each umbrella task would use `2 * 5 + 1 = 11` cores where the extra core is used by the master process.\n",
    "* Each umbrella task would use 5G of RAM (`5 * 1G`).\n",
    "* Each umbrella task would have a `walltime` of `1000 / 5 * 1 = 200` minutes (`walltime='03:20:00'`).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option `workdir`\n",
    "\n",
    "Default to current working directory.\n",
    "\n",
    "Option `workdir` controls the working directory of the task. For example, the following step downloads a file to the `resource_dir` using command `wget`.\n",
    "\n",
    "```python\n",
    "[10]\n",
    "\n",
    "task: workdir=resource_dir\n",
    "\n",
    "run:\n",
    "  wget a_url -O filename\n",
    "```\n",
    "\n",
    "Runtime option `workdir` will be translated to remote host if the task is executed remotely."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option `concurrent`\n",
    "\n",
    "Default to `True`.\n",
    "\n",
    "If the step process is repeated for multiple input groups (using input options `group_by` or `for_each`), all loop processes will by default be sent to the task engine to be executed in parallel (subject to `max_running_jobs` of individual task queue). If your tasks are sequential in nature (e.g. the next input group depends on the result of the current input group), you can set `concurrent=False`, in which case the next task will be generated and sent to the task queue only after the current one has been completed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option `shared`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SoS tasks are executed externally and by default does not return any value. Similar to the `shared` step option (that passes step variables to the workflow), you can use `shared` option to pass task variables to the step in which the task is defined.\n",
    "\n",
    "For example, the following script perform some simulations in 10 tasks and return the result by variable `rng`, which is then shared to the workflow by step option `shared` so that it can be available to the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "kernel": "sos"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 tasks completed.\n",
      "{0: 857, 1: 904, 2: 404, 3: 862, 4: 269, 5: 87, 6: 699, 7: 240, 8: 119, 9: 245}\n"
     ]
    }
   ],
   "source": [
    "%run\n",
    "[10 (simulate): shared='rng']\n",
    "input: for_each={'i': range(10)}\n",
    "task: shared='rng'\n",
    "print(\"${i}\")\n",
    "import random\n",
    "rng = random.randint(1, 1000)\n",
    "\n",
    "[20]\n",
    "print(rng)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to note that, however, because there can be multiple tasks in one step, the variables returned by tasks are dictionaries with loop index `_index` as keys."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also similar to step option `shared`, task option `shared` accepts a single variable (e.g. `rng`), a sequence of variables  (e.g. `('rng', 'sum')`), a dictionary of variable derived from an expression (e.g. `{'result': 'float(open(output).read())'}`, or sequences of names and variables. In the dictionary case, the values of the dictionary should be an expression (string), that will be evaluated upon the completion of the task, and assign to the specified variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option `env`\n",
    "\n",
    "The `env` option allow you to modify runtime environment, similar to the `env` parameter of the `subprocess.Popen` function. For example, you can execute your command with in a specific directory using\n",
    "\n",
    "```sos\n",
    "task:  env={'PATH': '/path/to/mycommand' + os.sep + os.environ['PATH']}\n",
    "run:\n",
    "   mycommand \n",
    "```\n",
    "\n",
    "Option `env` is NOT translated to remote host because it is of type directionay. The job template is usually a good place to set host-specific environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option `prepend_path`\n",
    "\n",
    "Option `prepend_path` is a shortcut to option `env` to prepend one (a string) or more (a list of strings) paths to system path. For example, the above example can be shortened to\n",
    "\n",
    "```sos\n",
    "task:  prepend_path='/path/to/mycommand'\n",
    "run:\n",
    "   mycommand \n",
    "```\n",
    "\n",
    "Option `prepend_path` is NOT translated to remote host because it is likely to be host specific."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "output_cache": "[]"
   },
   "source": [
    "### Option `active`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "output_cache": "[]"
   },
   "source": [
    "Option `active` specifies the active task within a input loop. It should be an index or a list of indexes when the task will be executed. Negative index is acceptable (e.g. task for only the last input loop will be executed with `active=-1`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Local and remote Targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local and remote targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All targets are by default `local`, in the sense that they are available locally. **Local targets will be synchronized to remote hosts to be processed by tasks**. Remote tasks are targets that reside on a remote host. **Remote targets will not be synchronized between local and remote hosts**. \n",
    "\n",
    "| | Local Targets | Remote Targets |\n",
    "|----| ---- |----|\n",
    "|location | locally | on remote host (although a remote host can be local) |\n",
    "|representation | w.r.t local file system | w.r.t remote file system |\n",
    "|Path translation | Translate to paths on different remote hosts if needed | Not translated |\n",
    "|processed in tasks | Yes | Yes |\n",
    "|processed outside of tasks | Yes | No |\n",
    "|Synchronized | Yes | No |\n",
    "\n",
    "In summary, remote targets allow you to use remote resources directly without synchronizing them to local host. SoS currently assumes that the targets are available on the host where the tasks are executed so there is no need to specify the host name for the targets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `local` and `remote` target types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Any targets can be specified as local or remote with syntax `local('input.txt')`, `remote('input.txt')` or `remote(R_Library('ggplot2'))`. For example, you could use the following step to process a large input file and only synchronize small output files to local desktop for further analysis:\n",
    "\n",
    "```\n",
    "[10]\n",
    "input: remote('/path/to/large/input/file')\n",
    "output: remote('large_output'), local('summary.stat')\n",
    "task:\n",
    "sh:\n",
    "    script to generate large_output and summary.stat\n",
    "    from large input files.\n",
    "```\n",
    "\n",
    "Here `local` can be ignored because it is the default mode for this step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `local` and `remote` section options"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have a large number of input and output files, it can be tedious to add `remote` to each target. In this case, you can use section option `remote` or `local` to set default mode for all targets in a step. For example, the above example can be written as\n",
    "\n",
    "```\n",
    "[10: remote]\n",
    "input: '/path/to/large/input/file'\n",
    "output: 'large_output', local('summary.stat')\n",
    "task:\n",
    "sh:\n",
    "    script to generate large_output and summary.stat\n",
    "    from large input files.\n",
    "```\n",
    "\n",
    "Here `local` is needed because the default mode of the targets are `remote`. You can of course do\n",
    "\n",
    "```\n",
    "[10: local]\n",
    "input: remote('/path/to/large/input/file')\n",
    "output: remote('large_output'), 'summary.stat'\n",
    "task:\n",
    "sh:\n",
    "    script to generate large_output and summary.stat\n",
    "    from large input files.\n",
    "```\n",
    "\n",
    "to use `local` as the default mode for targets in this step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "sos"
   },
   "source": [
    "### Global `remote` mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If all your input, dependent, and output files are on the remote server and you do not need to synchronize any of the files to the local machine, it makes sense to use `remote` as a global default mode. In this case, you can execute the entire workflow with option `-r` (`remote`), which essentially converts all input, dependent and output tasks in a workflow to `remote` targets, unless some sections or targets are specifically marked as `local`.\n",
    "\n",
    "For example, the workflow above can be written as\n",
    "\n",
    "```\n",
    "[10]\n",
    "input: '/path/to/large/input/file'\n",
    "output: 'large_output', local('summary.stat')\n",
    "task:\n",
    "sh:\n",
    "    script to generate large_output and summary.stat\n",
    "    from large input files.\n",
    "```\n",
    "\n",
    "and you can execute this workflow with all targets on `cluster` while retrieving only `summary.stat` to local machine\n",
    "\n",
    "```\n",
    "sos run myscript -r -q cluster\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "output_cache": "[]"
   },
   "source": [
    "## Commands and Options"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "output_cache": "[]"
   },
   "source": [
    "### Option `-q`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "output_cache": "[]"
   },
   "source": [
    "Option `-q` can be used by all task-related commands. It specifies a remote server or task queue, with detailed information defined in either global (`~/.sos/config.yml`) or local (`./config.yml`) configuration files. You can also save configurations to other configuration files and specify them using option `-c`. E.g.\n",
    "\n",
    "```bash\n",
    "sos run myscript -q shark -c shark.yml\n",
    "```\n",
    "uses definition of `shark` defined in `shark.yml`. You can define hosts in any of the configuration files and definitions loaded later override previous definitions. For example, you could define a set of test servers that is identical to definitions in global definition, but with `max_running_jobs` set to 1. You can then use\n",
    "\n",
    "```\n",
    "sos run myscript -q shark -c test.yml\n",
    "```\n",
    "for test runs and \n",
    "```\n",
    "sos run myscript -q shark\n",
    "```\n",
    "for regular execution.\n",
    "\n",
    "It is also possible for you to specify a host without configuration. Such hosts have default values\n",
    "\n",
    "```\n",
    "alias = host\n",
    "address = host\n",
    "queue_type = process\n",
    "path_map = []\n",
    "shared = []\n",
    "```\n",
    "\n",
    "which assumes that the host is a remote machine with identical but not shared file systems and without any task queue.\n",
    "\n",
    "Finally, if option `-q` is specified without value, the `sos` command will exit with a list of configured hosts so you can use command\n",
    "\n",
    "```\n",
    "sos status -q\n",
    "```\n",
    "to list all configured queued. Details of each queue will be displayed with option `-v3` or `-v4`.\n",
    "\n",
    "```\n",
    "sos status -q -v3\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "output_cache": "[]"
   },
   "source": [
    "### `sos run -q`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "output_cache": "[]"
   },
   "source": [
    "The `-q` option of command `sos run` (or `sos-runner`) sets the default task queue for all tasks. For example,\n",
    "\n",
    "```bash\n",
    "sos run myscript -q shark\n",
    "```\n",
    "\n",
    "would send all tasks in workflow `default` defined in `myscript.sos` to a task queue `shark`. Note that this option does not override option `queue` of steps so you could send some tasks to specific queues and all others to the default queue. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "output_cache": "[]"
   },
   "source": [
    "### `sos dryrun` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "output_cache": "[]"
   },
   "source": [
    "The dryrun running mode is very useful in checking if your scripts are correctly translated and if your machine settings are correct. **It is strongly recommended that you execute your script in dryrun mode before submitting them to remote hosts and/or task queues**.\n",
    "\n",
    "How tasks are executed depends a bit on your configuration but basically,\n",
    "\n",
    "1. For local tasks (`process` task engine), the tasks are executed directly with `sos execute task -n` (`-n` for dryrun mode).\n",
    "\n",
    "2. For direct remote execution (e.g. `sos run script -q server -n` with `queue_type` set to `process` (default)), the tasks will be generated and copied to the remote server, and will be executed with command `sos execute task -n`.\n",
    "\n",
    "3. For submitting to a PBS task queue (e.g. `sos run script -q pbs -n` with `queue_type` set to `pbs`), the tasks will be generated, copied to remote host. Job files will also be generated according to `job_template` and will be copied to the remote host. However, instead of using `submit_cmd` to submit the job to the PBS queue, **the job script will be executed directly on the head node**. It is therefore important for you to allow the jobs to be run in `dryrun` mode and complete in seconds. Otherwise your system admin would hunt you for running large jobs directly on head nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "output_cache": "[]"
   },
   "source": [
    "Dryrun mode allows you to test your workflow on a new server or task queue with the following approach:\n",
    "\n",
    "1. Define a `host_test` queue that is identical to `host` but with `max_running_jobs` set to 1.\n",
    "2. Run the script in dryrun mode, which would perform all file synchronization, task translation, submission steps, and execution steps. The only difference is that the scripts are printed instead of executed.\n",
    "    ```\n",
    "    sos run script -q host_test -n\n",
    "    ```\n",
    "    kill the command with `Ctrl-C` if the scripts are correct.\n",
    "    \n",
    "3. Submit one job to the task queue using command\n",
    "    ```\n",
    "    sos run script -q host_test\n",
    "    ```\n",
    "   kill the command if the task gets started and runs correctly.\n",
    "4. Submit the rest of the tasks using command\n",
    "    ```\n",
    "    sos run script -q host\n",
    "    ```\n",
    "    The job submitted at step 3 will be recognized and monitored. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `-w` and `-W`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Options `-w` (wait) and `-W` (no-wait) tells SoS to wait (or not wait) for the completion of tasks, which overrides the default wait mode for the task queue used (option `wait_for_task`). For example, a local process queue might be running in `wait` mode and a remote PBS queue is likely to be running in `no-wait` mode."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When a workflow is executed and reaches a point where all tasks have been submitted and no more step could be executed (namely the entire workflow is waiting for the completion of tasks), it will continue to wait in `wait` mode or quit in `no-wait` mode. Note that\n",
    "\n",
    "| feature| wait mode | no-wait mode|\n",
    "|---| ------- | -----|\n",
    "|when | `-w` or default for task queue with `wait_for_task=true` | `-W` or default for task queue with `wait_for_task=false`|\n",
    "|keyboard interruption | kills all running tasks | runing tasks are not affected |\n",
    "|task monitoring | continuous | external |\n",
    "|when tasks are completed | results synced, continue execution | results stay on  server (if remote) until the workflow resumes to fetch them|\n",
    "|resume execution | not needed | Command `sos resume` |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `sos resume`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Command `sos resume` resumes the execution of specified or last-executed workflow. In its simplest form, you can execute a workflow in no-wait mode\n",
    "\n",
    "```\n",
    "sos run workflow --options -W\n",
    "```\n",
    "\n",
    "if the command exit with a number of running tasks, you can run\n",
    "\n",
    "```\n",
    "sos resume\n",
    "```\n",
    "\n",
    "to resume the execution of workflow at any time. The command will exit immediately if all tasks are still running, and resume the execution if some tasks are completed. If more tasks are to be submitted, this command will exit again after submission more tasks unless option `-w` is specified.\n",
    "\n",
    "This command also accepts an option `--status` to check the status of pending workflows so if you are not sure if your workflows have been completed, you can execute\n",
    "\n",
    "```\n",
    "sos resume -s\n",
    "```\n",
    "\n",
    "to check the status of them. Note that `sos resume` does not have to be executed on the directory where the workflows were submitted so you can resume or check status of workflows from any directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "output_cache": "[]"
   },
   "source": [
    "###  `sos status`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Command \n",
    "\n",
    "```bash\n",
    "sos status [tasks] -q query\n",
    "```\n",
    "checkes the status of tasks. You can specify any number of first characeters of a task to specify a task, for example,\n",
    "\n",
    "```bash\n",
    "sos task 7\n",
    "sos task 77e\n",
    "sos task 7736e\n",
    "```\n",
    "would all work for a task with ID `77e36e7404cf6c2ef7079a09e84a4d6d`, but multiple tasks could be identifies if they share the same leading digits. Actually, \n",
    "\n",
    "```\n",
    "sos task \n",
    "```\n",
    "would match all tasks and list the status of all local tasks.\n",
    "\n",
    "Option `-q` specifies the task queue to monitor. For example,\n",
    "\n",
    "```\n",
    "sos status -q docker\n",
    "```\n",
    "\n",
    "would check the status of all tasks on a remote host `docker`. You can monitor the tasks on `docker` on any machine with defined host `docker`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "output_cache": "[]"
   },
   "source": [
    "###  `sos status -v -q`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "output_cache": "[]"
   },
   "source": [
    "Option `-v` controls the details of the output of command `sos status`. For example,\n",
    "\n",
    "```\n",
    "sos status e7404cf6c2 -v0\n",
    "```\n",
    "would print just the status of the task (e.g. `running`).\n",
    "\n",
    "```\n",
    "sos task 77e -v1\n",
    "```\n",
    "would print the task id and their status\n",
    "\n",
    "```\n",
    "77e36e7404cf6c2ef7079a09e84a4d6d    running\n",
    "77e3c2ef7079a236e7404cf6c2f343d3    completed\n",
    "```\n",
    "\n",
    "Option `-v0` and `-v1` could check the status of multiple tasks, as realized by SoS. Some tasks queues have their own task status command and option `-v2` (and upper) will use these commands (if specified) to check the status of the jobs. That is to say\n",
    "\n",
    "``` bash\n",
    "sos task 77e36 -v2\n",
    "```\n",
    "\n",
    "might return output of a command\n",
    "\n",
    "```\n",
    "qstat 18433\n",
    "```\n",
    "\n",
    "if the task has been submitted to a cluster named `shark` with a job id `18433`.\n",
    "\n",
    "If you would like to know more about the tasks,\n",
    "\n",
    "```bash\n",
    "sos task 77e36 -v3\n",
    "```\n",
    "\n",
    "would list the script the task is running and all variables in abbreviated format, and\n",
    "\n",
    "```bash\n",
    "sos task 77e36 -v4\n",
    "```\n",
    "would list all variables in complete form.\n",
    "\n",
    "Finally, using `-q` in combination with `-v` allows you to list the variables used in remote server.\n",
    "\n",
    "```bash\n",
    "sos task 77e36 -v4 -q linux\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "output_cache": "[]"
   },
   "source": [
    "### `sos kill` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "output_cache": "[]"
   },
   "source": [
    "Command\n",
    "\n",
    "```bash\n",
    "sos kill [tasks] [-q queue]\n",
    "```\n",
    "\n",
    "kills specified or all tasks on specified job queue `queue`. Because the same job could be executed on different queues (you have have done so), you will have to specify the correct queue name to kill the job on different queues.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "output_cache": "[]"
   },
   "source": [
    "### `sos execute`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "output_cache": "[]"
   },
   "source": [
    "Command \n",
    "````\n",
    "sos execute [tasks] [-q queue]\n",
    "```\n",
    "\n",
    "is the command that is used internally by `sos run` to execute tasks but you could use this command to execute tasks externally. For example, if a task failed on a server, you could use command\n",
    "\n",
    "```\n",
    "sos execute task_id -q server\n",
    "```\n",
    "\n",
    "to execute the command on another server. Note that `task_id` specifies a local task with local paths. The task will be converted to a remote task (with path names converted for that host) if `server` specifies a remote host. This makes it easy for you to re-submit tasks to the same server after changing server configuration, or submit the same task to a different server. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `sos purge`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Command `sos purge` is used to clear completed or failed tasks. You can remove tasks by age (`--age`) or status (option `--status`) from local or remote host (option `-q`). For example, command\n",
    "\n",
    "```\n",
    "sos purge -q cluster --all\n",
    "```\n",
    "removes all tasks from a remote cluster,\n",
    "\n",
    "```\n",
    "sos purge --age 2d\n",
    "```\n",
    "removes all tasks that are created more than 2 days ago, and\n",
    "\n",
    "```\n",
    "sos purge -q cluster -s completed\n",
    "```\n",
    "removes all completed tasks from the remote cluster.\n",
    "\n",
    "You can of course specify the IDs of tasks to be removed, e.g.\n",
    "\n",
    "```\n",
    "sos purge 38ef\n",
    "```\n",
    "\n",
    "removes all tasks with ID starting with `38ef`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A simple example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "output_cache": "[]"
   },
   "source": [
    "The following example demonstrates how to configure a remote host (with a PBS queue system) and submit your tasks to it. It can be tricky to set up public key access, configure host, and get everything working but you only need to do this once for each server or task queue that you would like to use. After that, you are free to submit your tasks to any of the servers without worrying about different file systems, task queues etc. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Host configuration\n",
    "\n",
    "`~/.sos/hosts.yml`\n",
    "\n",
    "```\n",
    "hosts:\n",
    "  nautilus:\n",
    "    address: nautilus.mdanderson.edu\n",
    "    paths:\n",
    "      home: /scratch/bcb/bpeng1\n",
    "    queue_type: pbs\n",
    "    status_check_interval: 30\n",
    "    job_template: |\n",
    "      #!/bin/bash\n",
    "      #PBS -N ${job_name}\n",
    "      #PBS -l nodes=1:ppn=${cores}\n",
    "      #PBS -l walltime=${walltime}\n",
    "      #PBS -l mem=${mem//1000000}GB\n",
    "      #PBS -o ${home_dir}/.sos/tasks/${task}.out\n",
    "      #PBS -e ${home_dir}/.sos/tasks/${task}.err\n",
    "      #PBS -m ae\n",
    "      #PBS -M bpeng@mdanderson.org\n",
    "      #PBS -v ${cur_dir}\n",
    "      cd ${cur_dir}\n",
    "      sos execute ${task} -v ${verbosity} -s ${sig_mode} \\\n",
    "        ${'--dryrun' if run_mode=='dryrun' else ''}\n",
    "    max_running_jobs: 100\n",
    "    submit_cmd: msub ${job_file}\n",
    "    status_cmd: qstat ${job_id}\n",
    "    kill_cmd: qdel ${job_id}\n",
    "  workstation:\n",
    "    paths:\n",
    "      home:/Users/bpeng1\n",
    "```\n",
    "\n",
    "`~/.sos/config.yml`\n",
    "\n",
    "```\n",
    "localhost: workstation\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test script\n",
    "\n",
    "```\n",
    "[10]\n",
    "input: for_each={'tid': range(10) }\n",
    "\n",
    "task: walltime='20m', mem='100M'\n",
    "\n",
    "run:\n",
    "    echo I am task ${tid}\n",
    "    sleep ${60  * (tid + 1)}\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "output_cache": "[]"
   },
   "source": [
    "#### Commands\n",
    "\n",
    "```\n",
    "sos run test -q nautilus\n",
    "sos status -q nautilus\n",
    "sos kill cb1 -q nautilus\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Select cell kernel",
  "kernelspec": {
   "display_name": "SoS",
   "language": "sos",
   "name": "sos"
  },
  "language_info": {
   "codemirror_mode": "sos",
   "file_extension": ".sos",
   "mimetype": "text/x-sos",
   "name": "sos",
   "nbconvert_exporter": "sos.jupyter.converter.SoS_Exporter",
   "pygments_lexer": "sos"
  },
  "sos": {
   "celltoolbar": true,
   "kernels": [
    [
     "Bash",
     "bash",
     "Bash",
     "#E6EEFF"
    ],
    [
     "Python2",
     "python2",
     "Python2",
     "#F6FAEA"
    ],
    [
     "JavaScript",
     "javascript",
     "JavaScript",
     "#00ff80"
    ],
    [
     "R",
     "ir",
     "R",
     "#FDEDEC"
    ],
    [
     "sas",
     "sas",
     "",
     ""
    ],
    [
     "Python3",
     "python3",
     "Python3",
     "#EAFAF1"
    ],
    [
     "SoS",
     "sos",
     "",
     ""
    ]
   ],
   "panel": {
    "displayed": true,
    "height": 0,
    "style": "side"
   }
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "172px",
    "width": "254px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_position": {
    "height": "675px",
    "left": "0px",
    "right": "1920px",
    "top": "106px",
    "width": "361px"
   },
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
