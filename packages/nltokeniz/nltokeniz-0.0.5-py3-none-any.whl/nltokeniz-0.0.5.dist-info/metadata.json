{"classifiers": ["Development Status :: 3 - Alpha", "Intended Audience :: Developers", "License :: Public Domain", "Natural Language :: English", "Natural Language :: Japanese", "Programming Language :: Python :: 3.4", "Programming Language :: Python :: 3.5", "Programming Language :: Python :: 3.6", "Topic :: Text Processing :: Linguistic"], "extensions": {"python.commands": {"wrap_console": {"nltokeniz": "nltokeniz.__main__:main"}}, "python.details": {"contacts": [{"email": "raviqqe@gmail.com", "name": "Yota Toyama", "role": "author"}], "document_names": {"description": "DESCRIPTION.rst"}, "project_urls": {"Home": "https://github.com/raviqqe/nltokeniz.py/"}}, "python.exports": {"console_scripts": {"nltokeniz": "nltokeniz.__main__:main"}}}, "extras": [], "generator": "bdist_wheel (0.29.0)", "license": "Public Domain", "metadata_version": "2.0", "name": "nltokeniz", "run_requires": [{"requires": ["iso639", "langdetect", "nlnormaliz", "nltk"]}], "summary": "Natural language tokenizer for documents in Python", "version": "0.0.5"}